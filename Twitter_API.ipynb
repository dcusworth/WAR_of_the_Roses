{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/matplotlib/__init__.py:872: UserWarning: axes.color_cycle is deprecated and replaced with axes.prop_cycle; please use the latter.\n",
      "  warnings.warn(self.msg_depr % (key, alt_key))\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import oauth2\n",
    "from twython import Twython\n",
    "import simplejson\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "from pyquery import PyQuery as pq\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import NoAlertPresentException\n",
    "import sys\n",
    "\n",
    "import unittest, time, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#APP_KEY = \"qtmevmQ18N1vyWTAXfxqmh4oN\"\n",
    "#APP_SECRET = \"MdZibormo3teZPTfMyeLEcuzMURHYidArOml0GtOQyrl6dI13R\"\n",
    "\n",
    "#access_token = '2694571580-Y8DsMjB0iMTGmm3Pwpo6IL3enhhFdAZQSXDIxO8'\n",
    "#access_secret = 'AYciwyU197r6adpNziDT8pB0tmT3bKIihMrx7SPfbofRO'\n",
    "\n",
    "#Define Twitter GET function using OAUTH2\n",
    "#Function from https://dev.twitter.com/oauth/overview/single-user\n",
    "#def oauth_req(url, key, secret, http_method=\"GET\", post_body=\"\", http_headers=None):\n",
    "#    consumer = oauth2.Consumer(key=APP_KEY, secret=APP_SECRET)\n",
    "#    token = oauth2.Token(key=key, secret=secret)\n",
    "#    client = oauth2.Client(consumer, token)\n",
    "#    resp, content = client.request( url, method=http_method, body=post_body, headers=http_headers )\n",
    "#    return content\n",
    "\n",
    "#twitter = Twython(APP_KEY, APP_SECRET, access_token, access_secret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We borrow heavily from http://stackoverflow.com/questions/12519074/scrape-websites-with-infinite-scrolling\n",
    "def scrape_page(since, until, contestant, \\\n",
    "                base_url=\"https://twitter.com/search?f=tweets&vertical=default&q=%23thebachelor\", \\\n",
    "                pages_to_scroll=3, ):\n",
    "    \n",
    "    #### Initiate Chrome Browser #######\n",
    "    #Must download ChromeDriver executable from https://sites.google.com/a/chromium.org/chromedriver/downloads\n",
    "    driver = webdriver.Chrome('/Users/dcusworth/chrome_driver/chromedriver') #Specify location of driver\n",
    "    driver.implicitly_wait(30)\n",
    "    verificationErrors = []\n",
    "    accept_next_alert = True\n",
    "\n",
    "    #Create URL that will get the text\n",
    "    ender = \"&src=typd\"\n",
    "    \n",
    "    #Use Twitter Sentiment Analysis - REMOVED as it may be underestimating tweets\n",
    "    #if is_happy:\n",
    "    #    sentiment = \"%20%3A)\"\n",
    "    #else:\n",
    "    #    sentiment = \"%20%3A(\"\n",
    "    \n",
    "    since_time = \"%20since%3A\" + str(since)\n",
    "    until_time = \"%20until%3A\" + str(until)\n",
    "    contestant_name = \"%20\" + contestant    \n",
    "        \n",
    "    final_url = base_url + contestant_name  + since_time + until_time + ender\n",
    "    #print final_url\n",
    "    \n",
    "    #Jump onto the webpage and scroll down\n",
    "    delay = 3\n",
    "    driver.get(final_url)\n",
    "    for i in range(1,pages_to_scroll):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(4)\n",
    "    html_source = driver.page_source\n",
    "    \n",
    "    #After scrolling enough times, get the text of the page\n",
    "    data = html_source.encode('utf-8')\n",
    "    driver.quit()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 1 Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "contestants = [\"Olivia\", \"Lauren B\", \"LB\", \"Caila\", \"Amber\", \"Jami\", \\\n",
    "               \"Jennifer\", \"Jubilee\", \"Amanda\", \"JoJo\", \"Leah\", \"Rachel\", \\\n",
    "               \"Samantha\", \"Jackie\", \"Haley\", \"Emily\", \"Shushanna\", \"Lauren H\", \\\n",
    "               \"Becca\", \"Mandi\", \"Lace\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "week1_tweets = {}\n",
    "\n",
    "for cont in contestants:\n",
    "    npage = scrape_page(\"2016-01-04\", \"2016-01-06\", cont, pages_to_scroll=100)\n",
    "\n",
    "    soup = BeautifulSoup(npage, \"html.parser\")\n",
    "    user_tweets = soup.find_all(\"p\", attrs={\"class\": \"TweetTextSize\"})\n",
    "\n",
    "    each_tweet = [uu.get_text() for uu in user_tweets]\n",
    "    N = len(each_tweet)\n",
    "    \n",
    "    week1_tweets.update({cont:N})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Convert to share\n",
    "week1_share = {}\n",
    "N = np.sum(week1_tweets.values())\n",
    "for cont in week1_tweets.keys():\n",
    "    week1_share.update({cont:week1_tweets[cont] / float(N)})\n",
    "\n",
    "#Normalize 0 to 1 scale\n",
    "tMAX = np.max(week1_tweets.values())\n",
    "tMIN = np.min(week1_tweets.values())\n",
    "week1_normalize = {}\n",
    "for cont in week1_tweets.keys():\n",
    "    week1_normalize.update({cont: (week1_tweets[cont] - tMIN) / float(tMAX - tMIN)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('week1.json', 'w') as fp:\n",
    "    json.dump(week1_share, fp)\n",
    "    \n",
    "with open('week1n.json', 'w') as fp:\n",
    "    json.dump(week1_normalize, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Amanda': 0.45792462570986059,\n",
       " 'Amber': 0.44243675787299946,\n",
       " 'Becca': 0.0,\n",
       " 'Caila': 0.050077439339184307,\n",
       " 'Emily': 0.1357769747031492,\n",
       " 'Haley': 0.059886422302529684,\n",
       " 'Jackie': 0.060402684563758392,\n",
       " 'Jami': 0.056272586473928757,\n",
       " 'Jennifer': 0.077955601445534331,\n",
       " 'JoJo': 0.15074858027878163,\n",
       " 'Jubilee': 0.45224574083634489,\n",
       " 'LB': 0.051109963861641711,\n",
       " 'Lace': 1.0,\n",
       " 'Lauren B': 0.34176561693340218,\n",
       " 'Lauren H': 0.021166752710376872,\n",
       " 'Leah': 0.1109963861641714,\n",
       " 'Mandi': 1.0,\n",
       " 'Olivia': 0.69024264326277751,\n",
       " 'Rachel': 0.15694372741352608,\n",
       " 'Samantha': 0.26897263810015487,\n",
       " 'Shushanna': 0.1481672689726381}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "week1_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
