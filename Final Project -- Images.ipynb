{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "\n",
    "from pyquery import PyQuery as pq\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "linkVec = []\n",
    "\n",
    "for thisPage in [3]:#[2,3,4,5]:\n",
    "    allLinkPage=requests.get(\"http://www.sirlinksalot.net/archives/thebachelor%d.html\" % (thisPage))\n",
    "    soup = BeautifulSoup(allLinkPage.text, \"html.parser\")\n",
    "    rows = soup.find_all(\"a\")\n",
    "    for row in rows:\n",
    "        rowString = str(row)\n",
    "        checkASCII = [ord(letter) for letter in rowString]\n",
    "        #print max(checkASCII),min(checkASCII)\n",
    "        if max(checkASCII)<127 and min(checkASCII)>9:\n",
    "        #    print max(checkASCII)<127 and min(checkASCII)>10\n",
    "            thisLink = str(row.get(\"href\"))\n",
    "            if \"sirlinksalot\" not in thisLink: #remove internal links\n",
    "                if \"clickbank.net\" not in thisLink: #remove ad links\n",
    "                    if \"pub43\" not in thisLink: #remove ad links\n",
    "                        if \"bilbo\" not in thisLink: #remove ad links\n",
    "                            if \"fastclick\" not in thisLink: #remove ad links\n",
    "                                if \"casalemedia\" not in thisLink: #remove ad links\n",
    "                                    linkVec.append(thisLink)\n",
    "                                \n",
    "\n",
    "\n",
    "#cleaner = lambda r: [int(r[0].get_text()), r[1].get_text(), r[2].get_text(), r[2].find(\"a\").get(\"href\")]\n",
    "#fields = [\"ranking\", \"title\", \"band_singer\", \"url\"]\n",
    "#songs = [dict(zip(fields, cleaner(row.find_all(\"td\")))) for row in rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1753\n"
     ]
    }
   ],
   "source": [
    "print len(linkVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sort our vector to group similar website roots\n",
    "linkVec = sorted(linkVec[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "archivedLinkVec = []\n",
    "\n",
    "count = 0;\n",
    "length = len(linkVec)\n",
    "for thisLink in linkVec:\n",
    "    allLinkPage=requests.get(\"https://web.archive.org/web/*/\" + thisLink)\n",
    "    soup = BeautifulSoup(allLinkPage.text, \"html.parser\")\n",
    "    if soup.find_all(\"div\", attrs={\"class\": \"date captures\"}):\n",
    "        row = soup.find_all(\"div\", attrs={\"class\": \"date captures\"})[0]\n",
    "        row = row.find(\"a\").get(\"href\")\n",
    "        archivedLinkVec.append(str(\"https://web.archive.org\" + row))\n",
    "    count = count+1\n",
    "    #print count, \" out of \", length, \" done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#archivedLinkVec[0:40]\n",
    "#https://web.archive.org/web/20130114020937/http://www.zap2it.com/news/pictures/zap-the-bachelor-meet-the-season-17-bachelorettes-20120925,0,7638604.photogallery?index=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#we can assume that each article from the Examiner website will have similar HTML formatting. \n",
    "#So let us also note the root page.\n",
    "rootPage = []\n",
    "for link in archivedLinkVec:\n",
    "    first = 50#link.find(\"www\") \n",
    "    last = link.find(\".com\")+4\n",
    "    rootPage.append(link[first:last])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#collect into a data frame\n",
    "linkDF = pd.DataFrame()\n",
    "linkDF['archived_link']=archivedLinkVec\n",
    "linkDF['root_page']= rootPage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myGroups = linkDF.groupby('root_page')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abclocal.go.com\n",
      "abcnews.go.com\n",
      "bachelor.blogs.pressdemocrat.com\n",
      "blog.syracuse.com\n",
      "blog.zap2it.com\n",
      "\n",
      "There are 5 unique groups.\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for group in linkDF.root_page.unique():\n",
    "    if group:\n",
    "        count = count+1\n",
    "        print group\n",
    "print\n",
    "print \"There are\",count, \"unique groups.\"\n",
    "    \n",
    "#this shows that the number of unique websites whose formatting we need to learn is pretty limited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z2 = myGroups.get_group(\"blog.zap2it.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#GOOD FOR SHEKNOWS.COM will later want to loop through all pages in linkDF\n",
    "testThis = \"https://web.archive.org/web/20120904010423/http://realitytvmagazine.sheknows.com/2012/09/02/arie-luyendyk-jr-confirms-that-he-will-not-be-the-next-bachelor/\"\n",
    "testPage=requests.get(testThis)\n",
    "soup = BeautifulSoup(testPage.text, \"html.parser\")\n",
    "rows = soup.find_all(\"img\")\n",
    "\n",
    "SK_mightHaveImage = []\n",
    "for row in rows:\n",
    "    if row.get(\"src\"):\n",
    "        SK_mightHaveImage.append(str(row.get(\"src\")))\n",
    "SK_hasImage = []\n",
    "for elem in SK_mightHaveImage:\n",
    "    if \"blogs.sheknows.com\" in elem:\n",
    "        if \".jpg\" in elem:\n",
    "            SK_hasImage.append(elem)\n",
    "SK_hasImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SEASON 17 BLUE BACKGROUND\n",
    "season17PicLinks=[]\n",
    "for i in range(2,9):\n",
    "    season17PicLinks.append(\"https://web.archive.org/web/20151113223542/http://www.trbimg.com/img-50620939/turbine/zap-the-bachelor-meet-the-season-17-bacheloret-00%d/600\" % i)\n",
    "for j in range(10,29):\n",
    "    season17PicLinks.append(\"https://web.archive.org/web/20151113223542/http://www.trbimg.com/img-50620939/turbine/zap-the-bachelor-meet-the-season-17-bacheloret-0%d/600\" % j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SEASON 18 BLUE BACKGROUND\n",
    "season18Links=[]\n",
    "for k in range(71,98):\n",
    "    season18Links.append(\"http://www.usmagazine.com/entertainment/pictures/the-bachelor-season-18-meet-juan-pablos-bachelorettes-2013412/343%d\" % k)\n",
    "    \n",
    "season18PicLinks = []\n",
    "for link in season18Links:\n",
    "    testPage=requests.get(link)\n",
    "    soup = BeautifulSoup(testPage.text, \"html.parser\")\n",
    "    rows = soup.find_all(\"img\")\n",
    "    for row in rows:\n",
    "        if row.get(\"src\"):\n",
    "            if \"18-meet-juan-pablos\" in row.get(\"src\"):\n",
    "                season18PicLinks.append(str(row.get(\"src\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SEASON 19 BLUE BACKGROUND\n",
    "season19Links=[]\n",
    "nameBank = [\"Whitney\",\"Becca\",\"Kaitlyn\",\"Jade\",\"Carly\",\"Britt\",\"Megan\",\"Kelsey\",\"Ashley-I\",\"Mackenzie\",\"Samantha\",\"Ashley-S\",\"Juelia\",\"Nikki\",\"Jillian\",\"Amber\",\"Tracy\",\"Trina\",\"Alissa\",\"Jordan\",\"Kimberly\",\"Tandra\",\"Tara\",\"Amanda\",\"Bo\",\"Brittany\",\"Kara\",\"Michelle\",\"Nicole\",\"Reegan\"]\n",
    "season19Links.append(\"http://www.usmagazine.com/entertainment/pictures/bachelor-season-19-chris-soules-bachelorettes-2014412/42609\")\n",
    "for k in range(10,39):\n",
    "    season19Links.append(\"http://www.usmagazine.com/entertainment/pictures/bachelor-season-19-chris-soules-bachelorettes-2014412/426%d\"%k)\n",
    "\n",
    "season19PicLinks = []\n",
    "for link in season19Links:\n",
    "    testPage=requests.get(link)\n",
    "    soup = BeautifulSoup(testPage.text, \"html.parser\")\n",
    "    rows = soup.find_all(\"img\")\n",
    "    for row in rows:\n",
    "        if row.get(\"src\"):\n",
    "            if \"19-chris-soules-bachelorettes\" in row.get(\"src\"):\n",
    "                season19PicLinks.append(str(row.get(\"src\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GOOD FOR ZAP2IT.COM will later want to loop through all pages in linkDF\n",
    "testThis = \"https://web.archive.org/web/20130114020937/http://www.zap2it.com/news/pictures/zap-the-bachelor-meet-the-season-17-bachelorettes-20120925,0,7638604.photogallery?index=2\"\n",
    "testPage=requests.get(testThis)\n",
    "soup = BeautifulSoup(testPage.text, \"html.parser\")\n",
    "rows = soup.find_all(\"img\")\n",
    "\n",
    "Z2_mightHaveImage = []\n",
    "for row in rows:\n",
    "    if row.get(\"src\"):\n",
    "        Z2_mightHaveImage.append(str(row.get(\"src\")))\n",
    "#Z2_mightHaveImage\n",
    "rows\n",
    "\n",
    "#Z2_hasImage = []\n",
    "#for elem in Z2_mightHaveImage:\n",
    "#    if \"blogs.sheknows.com\" in elem:\n",
    "#        if \".jpg\" in elem:\n",
    "#            Z2_hasImage.append(elem)\n",
    "#Z2_hasImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
