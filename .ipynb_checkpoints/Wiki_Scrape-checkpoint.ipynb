{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "from pyquery import PyQuery as pq\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get wiki for all bachelor seasons\n",
    "allseasons = requests.get(\"https://en.wikipedia.org/wiki/The_Bachelor_(U.S._TV_series)#Seasons\")\n",
    "#make beautiful soup element\n",
    "soup = BeautifulSoup(allseasons.text, \"html.parser\")\n",
    "\n",
    "#get the table cell that has links to each episode\n",
    "seasons = soup.find(\"table\", attrs={\"class\":\"navbox\"}).find(\"td\", attrs={\"class\":\"navbox-list navbox-odd hlist\"})\n",
    "seasons = seasons.find(\"div\", attrs={\"style\":\"padding:0em 0.25em\"}).find(\"ul\")\n",
    "\n",
    "urls = []                           #list of links to season-specific page\n",
    "seasonNums = []                     #list of seasons w/ wiki pages (no seasons 1-4 or 6-8)\n",
    "seasonNum = 1                       #season number\n",
    "for item in seasons.find_all(\"li\"): #for each item in list of seasons\n",
    "    if (seasonNum == 20):           #don't include season 20, b/c no contestants listed yet\n",
    "        break\n",
    "    season = item.find(\"a\")         #get url tag\n",
    "    if season is not None:          #if has url link, get url text\n",
    "        urls.append(\"\\\"https://en.wikipedia.org\" + season.get(\"href\") + \"\\\"\")\n",
    "        seasonNums.append(seasonNum) #add season number to list \n",
    "    seasonNum += 1\n",
    "    \n",
    "wikiPageText = []                   #init list of wiki site text, for all seasons\n",
    "for url in urls:\n",
    "    site = requests.get(url[1:-1])  #get web-site for that url\n",
    "    soup = BeautifulSoup(site.text, \"html.parser\") #make BS element\n",
    "    wikiPageText.append(soup)       #add web-site text to list\n",
    "\n",
    "wikiPages = dict(zip(seasonNums, wikiPageText)) #key=season, val=Soup Elem(wiki page text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lucy \n"
     ]
    }
   ],
   "source": [
    "# For each season in wiki, make list of dictionaries - one dictionary for each contestant.\n",
    "# dictionary name = seasonsDict\n",
    "#             key = season number\n",
    "#           value = list of dictionaries for that season (one for each contestant)\n",
    "#             \n",
    "# For contestant dictionaries:\n",
    "#            keys = name, age, hometown, occupation, elimination, season\n",
    "#          values = associated values to fields, as scraped from wiki\n",
    "#\n",
    "# To test contestant dictionaries:\n",
    "#         print seasonsDict[season][contestant][fieldname]\n",
    "#    eg:  print seasonsDict[9][10]['name']  -- get name for season 9, contestant 10\n",
    "#\n",
    "# Note: Wiki does not have pages dedicated to Seasons 1-4, or 6-8. Also, Wiki does not list\n",
    "# contestants for episode 20.  Those Seasons will be added to the dictionary later.\n",
    "\n",
    "seasonsDict = dict()                #key = season num, val=list of contestant dictionaries\n",
    "allContestants = dict()             #keys = name/age/etc, values = associated data\n",
    "\n",
    "for sn in seasonNums:\n",
    "    seasonPage = wikiPages[sn]      #get BS element for this season\n",
    "    seasonPage = seasonPage.find(\"div\", attrs={\"id\":\"content\"}).find(\"div\", attrs={\"id\":\"bodyContent\"})\n",
    "    seasonPage = seasonPage.find(\"div\", attrs={\"id\":\"mw-content-text\"})\n",
    "    seasonPage = seasonPage.find(\"table\", attrs={\"class\":\"wikitable sortable\"})\n",
    "    \n",
    "    listOfContestantDicts = []          #list of dicts for each contestant\n",
    "    \n",
    "    numtr = 0                           #num rows (one per contestant)\n",
    "    for tr in seasonPage.find_all(\"tr\"):#for each contestant listed,\n",
    "        if (numtr == 0):                #skip first row (column headers)\n",
    "            numtr += 1\n",
    "            continue\n",
    "\n",
    "        contestantDict = dict()         #init new dict for contestant\n",
    "        numtd = 0                       #column number\n",
    "        for td in tr.find_all(\"td\"):    #for each column of data,\n",
    "            \n",
    "            if (numtd == 0):\n",
    "                name = str(td.contents)\n",
    "                if (\"<b>\" in name):\n",
    "                    td.find(\"b\")\n",
    "                    name = str(td.contents)[4:-5]\n",
    "                if (\"[u'\" in name):                     #if in format \"[u'name']\",\n",
    "                    name = name.encode('utf8')[3:-2]    #format to get 'name'\n",
    "                if (\"<span class\" in name):\n",
    "                    td.find(\"span\", attrs={\"class\":\"nowrap\"})\n",
    "                    tag = \"<span class='nowrap'>\"       #start tag before name\n",
    "                    name = str(td.contents)[len(tag)+1:]#cut out start tag\n",
    "                    end = name.index(\"<\")               #get start point of end tag\n",
    "                    name = name[:end]                   #cut out end tag\n",
    "                    trashTag = \"style=\\\"display:none;\\\">\" #weird tag to cut from a name\n",
    "                    if (trashTag in name):\n",
    "                        name = name[(len(trashTag)+1):-1] \n",
    "                if (\"<sup\" in name):                    #if name has \"name', <sup ...\",\n",
    "                    end2 = name.index(\"<sup\")           #format to get name\n",
    "                    name = name[:end2-3]\n",
    "                contestantDict['name'] = name           #add name to dict\n",
    "\n",
    "            if (numtd == 1):\n",
    "                age = str(td.contents)\n",
    "                if (\"<b>\" in age):\n",
    "                    td.find(\"b\")\n",
    "                    age = str(td.contents)[4:-5]\n",
    "                if (\"[u'\" in age):                      \n",
    "                    age = age.encode('utf8')[3:5]       \n",
    "                contestantDict['age'] = age\n",
    "            if (numtd == 2):\n",
    "                td.find(\"a\")\n",
    "                home = td.get(\"href\")       \n",
    "                home = td.get_text(\"title\")\n",
    "                if (\"title\" in home):                #format oddity in season 19, contest 1\n",
    "                    indx = home.index(\"title\")\n",
    "                    home = home[:indx]\n",
    "                contestantDict['hometown'] = home\n",
    "            if (numtd == 3):\n",
    "                job = str(td.contents)\n",
    "                if (\"<b>\" in job):\n",
    "                    td.find(\"b\")\n",
    "                    job = str(td.contents)[4:-5]\n",
    "                if (\"[u'\" in job):                     \n",
    "                    job = job.encode('utf8')[3:-2]      \n",
    "                contestantDict['occupation'] = job   \n",
    "            if (numtd == 4):\n",
    "                elim = str(td.contents)\n",
    "                if (\"<b>\" in elim):\n",
    "                    td.find(\"b\")\n",
    "                    elim = str(td.contents)[4:-5]\n",
    "                if (\"[u'\" in elim):                      \n",
    "                    elim = elim.encode('utf8')[3:-2]      \n",
    "                contestantDict['eliminated'] = elim\n",
    "            numtd += 1\n",
    "        numtr += 1\n",
    "        contestantDict['season'] = sn   #include season num in dict\n",
    "        \n",
    "        listOfContestantDicts.append(contestantDict) #add dict to list of dicts\n",
    "        \n",
    "    seasonsDict[sn] = listOfContestantDicts  #key = season num, val=list of contestant dicts\n",
    "\n",
    "#no seasons: 1-4, 6-8, 20\n",
    "#to test: print seasonsDict [season][contestant][fieldname], eg:\n",
    "#print seasonsDict[9][10]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get data for seasons 1-4 and 6-8\n",
    "#possible source:  http://abc.go.com/primetime/bachelor/index?pn=photos#t=31304\n",
    "\n",
    "#good source: http://www.realitywanted.com/shows/the-bachelor/season-8-paris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "# Get data for Season 8, add to dictionary\n",
    "\n",
    "def getSeason8(season8Site):\n",
    "    \n",
    "    #get site with Bachelor Season 8 Contestants\n",
    "    seasonEight = requests.get(season8Site)\n",
    "    #make beautiful soup element\n",
    "    season8= BeautifulSoup(seasonEight.text, \"html.parser\")\n",
    "\n",
    "    #get the table cell that has links to each episode\n",
    "    eight = season8.find(\"body\", attrs={\"id\":\"imagegalleryIndexPage\"})\n",
    "    eight = eight.find(\"main\", attrs={\"id\":\"main\"})\n",
    "    eight = eight.find(\"div\", attrs={\"class\":\"container\"})\n",
    "    eight = eight.find_all(\"div\", attrs={\"class\":\"row\"})[1]\n",
    "    eight = eight.find(\"div\", attrs={\"class\":\"col col-11\"}).find(\"div\", attrs={\"class\":\"row\"})\n",
    "    eight = eight.find(\"div\", attrs={\"class\":\"col col-8\"})\n",
    "    eight = eight.find(\"div\", attrs={\"class\":\"content widget gallery-index-content\"})\n",
    "    eight = eight.find(\"ul\")\n",
    "\n",
    "\n",
    "    urls8 = []                       #list of urls for season 8 contestant pages\n",
    "    for item in eight.find_all(\"li\"):#for each contestant in list of season 8 contestants\n",
    "        url8 = item.find(\"a\")       #get url tag\n",
    "        if url8 is not None:         #if has url link, get url \n",
    "            urls8.append(\"\\\"http://realitytv.about.com\" + url8.get(\"href\") + \"\\\"\")\n",
    "\n",
    "    cont8Sites = []                  #list of soup objects for season 8 contestant sites\n",
    "    for link in urls8:\n",
    "        site8 = requests.get(link[1:-1]) \n",
    "        soup8 = BeautifulSoup(site8.text, \"html.parser\") #get soup element\n",
    "        cont8Sites.append(soup8)     #add soup element to list\n",
    "\n",
    "    for cont8 in cont8Sites:\n",
    "        c8 = cont8.find(\"body\", attrs={\"id\":\"imagegalleryPage\"})\n",
    "        c8 = c8.find(\"main\", attrs={\"class\":\"slab\"})\n",
    "        c8 = c8.find(\"div\", attrs={\"class\":\"container\"})\n",
    "        c8 = c8.find_all(\"div\", attrs={\"class\":\"row\"})[1]\n",
    "        c8 = c8.find(\"div\", attrs={\"class\":\"col col-11\"})\n",
    "        c8 = c8.find(\"div\", attrs={\"id\":\"contentIntro\"})\n",
    "        c8 = c8.find(\"div\", attrs={\"class\":\"row\"})\n",
    "        c8 = c8.find(\"div\", attrs={\"class\":\"col col-6\"})\n",
    "        c8 = c8.find(\"div\", attrs={\"class\":\"muted subheading\"}).getText()\n",
    "    \n",
    "        #get name\n",
    "        firstComma = c8.index(',')\n",
    "        contestantDict['name'] = c8[:firstComma]   #add name to dict\n",
    "    \n",
    "        #get age\n",
    "        substrC8 = c8[firstComma+2:]\n",
    "        secondComma = substrC8.index(',')\n",
    "        contestantDict['age'] = substrC8[:secondComma]\n",
    "            \n",
    "        #get job\n",
    "        if (\"is a\" in c8):\n",
    "            jobtag = \"is a \"\n",
    "        if (\"is an\" in c8):\n",
    "            jobtag = \"is an \"\n",
    "        jobIndex = c8.index(jobtag)  \n",
    "        contestantDict['occupation'] = c8[(jobIndex+len(jobtag)):(c8.index(\"who\"))]   #add name to dict\n",
    "\n",
    "        #get hometown\n",
    "        hometag = \"resides in \"\n",
    "        homeIndex = c8.index(hometag)\n",
    "        contestantDict['hometown'] = c8[(homeIndex+len(hometag)):-1]\n",
    "    \n",
    "        #add season\n",
    "        contestantDict['season'] = c8[8]\n",
    "\n",
    "#call 'getSeason8' to scrape data from both sites that have season 8 data\n",
    "getSeason8(\"http://realitytv.about.com/od/thebachelor8/ig/Ladies-of-The-Bachelor--Paris/\")\n",
    "getSeason8(\"http://realitytv.about.com/od/thebachelor8/ig/Ladies-of-The-Bachelor--Paris/index.01.htm#step-heading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
