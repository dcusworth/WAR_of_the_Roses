{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import oauth2\n",
    "from twython import Twython\n",
    "import simplejson\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "from pyquery import PyQuery as pq\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import datetime\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import NoAlertPresentException\n",
    "import sys\n",
    "\n",
    "import unittest, time, re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API Method is good, but only gives us very recent twitter data. Below is an example of the type of code we would use to interact with the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#APP_KEY = \"qtmevmQ18N1vyWTAXfxqmh4oN\"\n",
    "#APP_SECRET = \"MdZibormo3teZPTfMyeLEcuzMURHYidArOml0GtOQyrl6dI13R\"\n",
    "\n",
    "#access_token = '2694571580-Y8DsMjB0iMTGmm3Pwpo6IL3enhhFdAZQSXDIxO8'\n",
    "#access_secret = 'AYciwyU197r6adpNziDT8pB0tmT3bKIihMrx7SPfbofRO'\n",
    "\n",
    "#twitter = Twython(APP_KEY, APP_SECRET, access_token, access_secret)\n",
    "#search_results = oauth_req('https://api.twitter.com/1.1/statuses/home_timeline.json', \\\n",
    "                          access_token, access_secret)\n",
    "\n",
    "#for tweet in search_results[\"statuses\"]:\n",
    "#    print tweet[\"text\"]\n",
    "\n",
    "#Define Twitter GET function using OAUTH2\n",
    "#Function from https://dev.twitter.com/oauth/overview/single-user\n",
    "#def oauth_req(url, key, secret, http_method=\"GET\", post_body=\"\", http_headers=None):\n",
    "#    consumer = oauth2.Consumer(key=APP_KEY, secret=APP_SECRET)\n",
    "#    token = oauth2.Token(key=key, secret=secret)\n",
    "#    client = oauth2.Client(consumer, token)\n",
    "#    resp, content = client.request( url, method=http_method, body=post_body, headers=http_headers )\n",
    "#    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Web Scrape of Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual Scraping of Twitter presents two challenges:\n",
    "\n",
    "1) Twitter uses JavaScript for interactive webpage scrolling. If a search produces multiple results, once a reader gets to the end of a page, instead of being prompted with a \"Next Page\" link, twitter automatically queries it's JSON backend and dynamically loads the page. \n",
    "\n",
    "To work around this issue, we use the package Selenium which mimics \"scrolling\" the webpage for us. After scrolling though a set number of pages, we extract the HTML from the page, as suffient XHR requests have been made by Twitter.\n",
    "\n",
    "2) Manual page data is not in nice JSON format, so we must use html parsing to get at the data.\n",
    "\n",
    "Since we are interested in the positive/negative vibes of a tweet, we use Twitter's sentiment analysis in our search queries for a particular contestant. Then all we need to do is count the number of tweet tags that we scraped.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create Function to Scrape Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We borrow heavily from http://stackoverflow.com/questions/12519074/scrape-websites-with-infinite-scrolling\n",
    "def scrape_page(since, until, is_happy, contestant, \\\n",
    "                base_url=\"https://twitter.com/search?f=tweets&vertical=default&q=%23thebachelor\", \\\n",
    "                pages_to_scroll=3, ):\n",
    "    \n",
    "    #### Initiate Chrome Browser #######\n",
    "    #Must download ChromeDriver executable from https://sites.google.com/a/chromium.org/chromedriver/downloads\n",
    "    driver = webdriver.Chrome('/Users/dcusworth/chrome_driver/chromedriver') #Specify location of driver\n",
    "    driver.implicitly_wait(30)\n",
    "    verificationErrors = []\n",
    "    accept_next_alert = True\n",
    "\n",
    "    #Create URL that will get the text\n",
    "    ender = \"&src=typd\"\n",
    "    if is_happy:\n",
    "        sentiment = \"%20%3A)\"\n",
    "    else:\n",
    "        sentiment = \"%20%3A(\"\n",
    "    \n",
    "    since_time = \"%20since%3A\" + str(since)\n",
    "    until_time = \"%20until%3A\" + str(until)\n",
    "    contestant_name = \"%20\" + contestant    \n",
    "        \n",
    "    final_url = base_url + contestant_name + sentiment + since_time + until_time + ender\n",
    "    print final_url\n",
    "    \n",
    "    #Jump onto the webpage and scroll down\n",
    "    delay = 3\n",
    "    driver.get(final_url)\n",
    "    for i in range(1,pages_to_scroll):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(4)\n",
    "    html_source = driver.page_source\n",
    "    \n",
    "    #After scrolling enough times, get the text of the page\n",
    "    data = html_source.encode('utf-8')\n",
    "    driver.quit()\n",
    "\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Retrive Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load in scraped Wikipedia Data that gives us a contestant's name and dates they appeared on the Bachelor. For each season/contestant pair, we create a dataframe of episode date, positive tweets, and negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://twitter.com/search?f=tweets&vertical=default&q=%23thebachelor%20Emily%20%3A)%20since%3A2012-01-01%20until%3A2012-01-04&src=typd\n",
      "https://twitter.com/search?f=tweets&vertical=default&q=%23thebachelor%20Emily%20%3A(%20since%3A2012-01-01%20until%3A2012-01-04&src=typd\n",
      "https://twitter.com/search?f=tweets&vertical=default&q=%23thebachelor%20Emily%20%3A)%20since%3A2012-01-08%20until%3A2012-01-11&src=typd\n",
      "https://twitter.com/search?f=tweets&vertical=default&q=%23thebachelor%20Emily%20%3A(%20since%3A2012-01-08%20until%3A2012-01-11&src=typd\n",
      "https://twitter.com/search?f=tweets&vertical=default&q=%23thebachelor%20Emily%20%3A)%20since%3A2012-01-15%20until%3A2012-01-18&src=typd\n",
      "https://twitter.com/search?f=tweets&vertical=default&q=%23thebachelor%20Emily%20%3A(%20since%3A2012-01-15%20until%3A2012-01-18&src=typd\n",
      "https://twitter.com/search?f=tweets&vertical=default&q=%23thebachelor%20Emily%20%3A)%20since%3A2012-01-22%20until%3A2012-01-25&src=typd\n",
      "https://twitter.com/search?f=tweets&vertical=default&q=%23thebachelor%20Emily%20%3A(%20since%3A2012-01-22%20until%3A2012-01-25&src=typd\n",
      "https://twitter.com/search?f=tweets&vertical=default&q=%23thebachelor%20Emily%20%3A)%20since%3A2012-01-29%20until%3A2012-02-01&src=typd\n",
      "https://twitter.com/search?f=tweets&vertical=default&q=%23thebachelor%20Emily%20%3A(%20since%3A2012-01-29%20until%3A2012-02-01&src=typd\n",
      "https://twitter.com/search?f=tweets&vertical=default&q=%23thebachelor%20Emily%20%3A)%20since%3A2012-02-05%20until%3A2012-02-08&src=typd\n",
      "https://twitter.com/search?f=tweets&vertical=default&q=%23thebachelor%20Emily%20%3A(%20since%3A2012-02-05%20until%3A2012-02-08&src=typd\n",
      "{datetime.datetime(2012, 1, 16, 0, 0): {'Emily': {'sad': 13, 'happy': 13}}, datetime.datetime(2012, 1, 9, 0, 0): {'Emily': {'sad': 3, 'happy': 3}}, datetime.datetime(2012, 1, 2, 0, 0): {'Emily': {'sad': 2, 'happy': 2}}, datetime.datetime(2012, 2, 6, 0, 0): {'Emily': {'sad': 2, 'happy': 2}}, datetime.datetime(2012, 1, 30, 0, 0): {'Emily': {'sad': 6, 'happy': 6}}, datetime.datetime(2012, 1, 23, 0, 0): {'Emily': {'sad': 3, 'happy': 3}}}\n"
     ]
    }
   ],
   "source": [
    "#Start with a test - Season 16, contestant Emily\n",
    "contestant = \"Emily\"\n",
    "episode_dates = ['2012-01-02', '2012-01-09', '2012-01-16', '2012-01-23', '2012-01-30', '2012-02-06']\n",
    "\n",
    "dats = [datetime.datetime.strptime(idate, '%Y-%m-%d') for idate in episode_dates]\n",
    "\n",
    "result_dict = {}\n",
    "for run_date in dats:\n",
    "    #Make time range\n",
    "    start_time = run_date +  datetime.timedelta(days=-1)\n",
    "    end_time = run_date +  datetime.timedelta(days=2)\n",
    "    \n",
    "    #Find all positive tweets\n",
    "    happy_time = scrape_page(since=start_time.strftime('%Y-%m-%d'), until=end_time.strftime('%Y-%m-%d'), \\\n",
    "                             is_happy=True, contestant=contestant)\n",
    "    soup = BeautifulSoup(happy_time, \"html.parser\")\n",
    "    happy_tweets = len(soup.find_all(\"p\", attrs={\"class\": \"TweetTextSize\"}))\n",
    "    \n",
    "    #Find all sad tweets\n",
    "    sad_time = scrape_page(since=start_time.strftime('%Y-%m-%d'), until=end_time.strftime('%Y-%m-%d'), \\\n",
    "                             is_happy=False, contestant=contestant)\n",
    "    soup = BeautifulSoup(sad_time, \"html.parser\")\n",
    "    sad_tweets = len(soup.find_all(\"p\", attrs={\"class\": \"TweetTextSize\"}))\n",
    "    \n",
    "    #Save the results to a dictionary\n",
    "    ep_dict = {contestant:{\"happy\":happy_tweets, \"sad\":sad_tweets}}\n",
    "    result_dict[run_date.strftime('%Y-%m-%d')] = ep_dict\n",
    "    \n",
    "print result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
