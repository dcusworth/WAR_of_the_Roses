{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Twitter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import oauth2\n",
    "from twython import Twython\n",
    "import simplejson\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "from pyquery import PyQuery as pq\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import NoAlertPresentException\n",
    "import sys\n",
    "\n",
    "import unittest, time, re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API Method is good, but only gives us very recent twitter data. Below is an example of the type of code we would use to interact with the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#APP_KEY = \"qtmevmQ18N1vyWTAXfxqmh4oN\"\n",
    "#APP_SECRET = \"MdZibormo3teZPTfMyeLEcuzMURHYidArOml0GtOQyrl6dI13R\"\n",
    "\n",
    "#access_token = '2694571580-Y8DsMjB0iMTGmm3Pwpo6IL3enhhFdAZQSXDIxO8'\n",
    "#access_secret = 'AYciwyU197r6adpNziDT8pB0tmT3bKIihMrx7SPfbofRO'\n",
    "\n",
    "#twitter = Twython(APP_KEY, APP_SECRET, access_token, access_secret)\n",
    "#search_results = oauth_req('https://api.twitter.com/1.1/statuses/home_timeline.json', \\\n",
    "#                          access_token, access_secret)\n",
    "\n",
    "#for tweet in search_results[\"statuses\"]:\n",
    "#    print tweet[\"text\"]\n",
    "\n",
    "#Define Twitter GET function using OAUTH2\n",
    "#Function from https://dev.twitter.com/oauth/overview/single-user\n",
    "#def oauth_req(url, key, secret, http_method=\"GET\", post_body=\"\", http_headers=None):\n",
    "#    consumer = oauth2.Consumer(key=APP_KEY, secret=APP_SECRET)\n",
    "#    token = oauth2.Token(key=key, secret=secret)\n",
    "#    client = oauth2.Client(consumer, token)\n",
    "#    resp, content = client.request( url, method=http_method, body=post_body, headers=http_headers )\n",
    "#    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Web Scrape of Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual Scraping of Twitter presents two challenges:\n",
    "\n",
    "1) Twitter uses JavaScript for interactive webpage scrolling. If a search produces multiple results, once a reader gets to the end of a page, instead of being prompted with a \"Next Page\" link, twitter automatically queries it's JSON backend and dynamically loads the page. \n",
    "\n",
    "To work around this issue, we use the package Selenium which mimics \"scrolling\" the webpage for us. After scrolling though a set number of pages, we extract the HTML from the page, as suffient XHR requests have been made by Twitter.\n",
    "\n",
    "2) Manual page data is not in nice JSON format, so we must use html parsing to get at the data.\n",
    "\n",
    "Since we are interested in the positive/negative vibes of a tweet, we use Twitter's sentiment analysis in our search queries for a particular contestant. Then all we need to do is count the number of tweet tags that we scraped.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create Function to Scrape Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We borrow heavily from http://stackoverflow.com/questions/12519074/scrape-websites-with-infinite-scrolling\n",
    "def scrape_page(since, until, contestant, \\\n",
    "                base_url=\"https://twitter.com/search?f=tweets&vertical=default&q=%23thebachelor\", \\\n",
    "                pages_to_scroll=3, ):\n",
    "    \n",
    "    #### Initiate Chrome Browser #######\n",
    "    #Must download ChromeDriver executable from https://sites.google.com/a/chromium.org/chromedriver/downloads\n",
    "    driver = webdriver.Chrome('/Users/dcusworth/chrome_driver/chromedriver') #Specify location of driver\n",
    "    driver.implicitly_wait(30)\n",
    "    verificationErrors = []\n",
    "    accept_next_alert = True\n",
    "\n",
    "    #Create URL that will get the text\n",
    "    ender = \"&src=typd\"\n",
    "    \n",
    "    #Use Twitter Sentiment Analysis - REMOVED as it may be underestimating tweets\n",
    "    #if is_happy:\n",
    "    #    sentiment = \"%20%3A)\"\n",
    "    #else:\n",
    "    #    sentiment = \"%20%3A(\"\n",
    "    \n",
    "    since_time = \"%20since%3A\" + str(since)\n",
    "    until_time = \"%20until%3A\" + str(until)\n",
    "    contestant_name = \"%20\" + contestant    \n",
    "        \n",
    "    final_url = base_url + contestant_name  + since_time + until_time + ender\n",
    "    #print final_url\n",
    "    \n",
    "    #Jump onto the webpage and scroll down\n",
    "    delay = 3\n",
    "    driver.get(final_url)\n",
    "    for i in range(1,pages_to_scroll):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(4)\n",
    "    html_source = driver.page_source\n",
    "    \n",
    "    #After scrolling enough times, get the text of the page\n",
    "    data = html_source.encode('utf-8')\n",
    "    driver.quit()\n",
    "\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Retrive Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load in scraped Wikipedia Data that gives us a contestant's name and dates they appeared on the Bachelor. For each season/contestant pair, we create a dataframe of episode date, positive tweets, and negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load Contestant Name Data from wiki scrape\n",
    "with open(\"tempdata/seasonsDict.json\") as json_file:\n",
    "    wiki_data = json.load(json_file)\n",
    "\n",
    "#Fix known formatting problems:\n",
    "wiki_data['19'][19]['eliminated'] = u'Eliminated in week 2'\n",
    "wiki_data['19'][20]['eliminated'] = u'Eliminated in week 1'\n",
    "\n",
    "w19 = []\n",
    "for ww in wiki_data['19'][0:29]:\n",
    "    w19.append(ww)\n",
    "    \n",
    "wiki_data['19'] = w19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Scrape Web to find the airdates of each episode\n",
    "#Use http://epguides.com/Bachelor/\n",
    "sdat = requests.get(\"http://epguides.com/Bachelor/\")\n",
    "\n",
    "#Parse through Beautriful Soup\n",
    "ssoup = BeautifulSoup(sdat.text, \"html.parser\")\n",
    "\n",
    "#Get all episode text in rows\n",
    "row_text = ssoup.find_all(\"pre\")[0]\n",
    "\n",
    "uurls = []\n",
    "ep_nam = []\n",
    "for r in row_text.find_all(\"a\"):\n",
    "    if \"Week\" in r.get_text():\n",
    "        uurls.append(r.get(\"href\"))\n",
    "        ep_nam.append(r.get_text())\n",
    "        \n",
    "#Fix Season 19 episode problems\n",
    "ep_nam[140:] = [ee + \" (S19)\" for ee in ep_nam[140:]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:15: DeprecationWarning: converting an array with ndim > 0 to an index will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "good_dates = []\n",
    "\n",
    "for uurl in uurls:\n",
    "    time.sleep(1)\n",
    "    #Open up subpages\n",
    "    subpage = requests.get(uurl)\n",
    "    soup2 = BeautifulSoup(subpage.text, \"html.parser\")\n",
    "    \n",
    "    #Find box with date in it\n",
    "    pars = soup2.find_all(\"br\")\n",
    "    pp = pars[0].get_text().split()\n",
    "    pind = [\"Airdate\" in d for d in pp]\n",
    "    \n",
    "    #Convert date from page into usable date\n",
    "    date_string = \"-\".join(pp[np.where(pind)[0]+1: np.where(pind)[0]+4])\n",
    "    date_string = re.sub(\",\", \"\",date_string)\n",
    "    date_object = datetime.datetime.strptime(date_string, \"%b-%d-%Y\")\n",
    "    good_dates.append(date_object.strftime(\"%Y-%m-%d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Extract the Season Number\n",
    "season_num = []\n",
    "for ee in ep_nam:\n",
    "    start_string = ee.find(\"(\")\n",
    "    season_num.append(int(ee[(start_string+2):(len(ee)-1)]))\n",
    "\n",
    "#Count up Episode Numbers\n",
    "ep_num = []\n",
    "start_val = 0\n",
    "season_start = 1\n",
    "for i in range(len(season_num)-1):\n",
    "    if season_num[i] == season_start:\n",
    "        start_val += 1\n",
    "        ep_num.append(start_val)\n",
    "    else:\n",
    "        season_start += 1\n",
    "        start_val = 1\n",
    "        ep_num.append(start_val)\n",
    "\n",
    "ep_num.append(ep_num[-1] + 1)\n",
    "\n",
    "#Put Season / Episodes / Dates into a Pandas Dataframe\n",
    "date_guide = pd.concat([pd.Series(season_num, name=\"Season\"), pd.Series(ep_num, name=\"Episode\"), \\\n",
    "                        pd.Series(good_dates,name=\"Date\")], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Use Date Guide + Wiki info to set up inputs to scrape_page\n",
    "#For a given Season, get all contestant names\n",
    "#For each contestant find how many episodes they were on (minus their elimination episode)\n",
    "#For each episode, count positive / negative tweets they received\n",
    "#Output a dictionary with the Season as Key, and a dictionary of of each contestant's pos/neg splits as values\n",
    "\n",
    "def scrape_season_tweets(season):\n",
    "    \n",
    "    season_dat = wiki_data[str(season)]\n",
    "    all_eps = date_guide[date_guide.Season == season]\n",
    "    result_dict = {}\n",
    "    \n",
    "    for sd in season_dat:\n",
    "        #Get contestant's name\n",
    "        cnam = sd[\"name\"]          \n",
    "        \n",
    "        if len(cnam.split(\">\")) > 1:\n",
    "            cnam2 = cnam.split(\">\")[1]\n",
    "            contestant = cnam2.encode(\"utf-8\").split(\" \")[0]\n",
    "        else:\n",
    "            contestant = cnam.encode(\"utf-8\").split(\" \")[0]\n",
    "        \n",
    "        for ch in [\"[\", \"]\", \"u\\\"\",\"<\",\">\"]:\n",
    "            contestant = contestant.replace(ch, \"\")\n",
    "        print contestant\n",
    "\n",
    "        #Find week they are elminated, and then select weeks to run scraper\n",
    "        elim = sd['eliminated']\n",
    "        if (\"Win\" in elim) | (\"Run\" in elim):\n",
    "            elim_week = all_eps.shape[0] - 1\n",
    "            eweek = all_eps.iloc[0:elim_week]\n",
    "            use_dats = eweek[\"Date\"]\n",
    "        else:\n",
    "            elim_week = int(elim[(len(elim)-1):len(elim)]) - 1\n",
    "            eweek = all_eps.iloc[0:elim_week]\n",
    "            use_dats = eweek[\"Date\"]\n",
    "\n",
    "        dats = [datetime.datetime.strptime(idate, '%Y-%m-%d') for idate in use_dats]\n",
    "\n",
    "        #For each date, run scraper, save in dictionary\n",
    "        ep_dict = []\n",
    "        if len(dats)==0 | (\"href\" in contestant):\n",
    "            result_dict[contestant] = None\n",
    "        else:\n",
    "            for run_date in dats:\n",
    "                #Make time range\n",
    "                start_time = run_date +  datetime.timedelta(days=-1)\n",
    "                end_time = run_date +  datetime.timedelta(days=2)\n",
    "\n",
    "                #Collect all tweets\n",
    "                tweet_page = scrape_page(since=start_time.strftime('%Y-%m-%d'), until=end_time.strftime('%Y-%m-%d'), \\\n",
    "                                        contestant=contestant, pages_to_scroll=10)\n",
    "                soup = BeautifulSoup(tweet_page, \"html.parser\")\n",
    "                user_tweets = soup.find_all(\"p\", attrs={\"class\": \"TweetTextSize\"})\n",
    "                \n",
    "                each_tweet = [uu.get_text() for uu in user_tweets]\n",
    "                            \n",
    "                #FOLLOWING CODE if doing Twitter-built-in sentiment analysis\n",
    "                #Find all positive tweets\n",
    "                #happy_time = scrape_page(since=start_time.strftime('%Y-%m-%d'), until=end_time.strftime('%Y-%m-%d'), \\\n",
    "                #                         is_happy=True, contestant=contestant)\n",
    "                #soup = BeautifulSoup(happy_time, \"html.parser\")\n",
    "                #happy_tweets = len(soup.find_all(\"p\", attrs={\"class\": \"TweetTextSize\"}))\n",
    "\n",
    "                #Find all sad tweets\n",
    "                #sad_time = scrape_page(since=start_time.strftime('%Y-%m-%d'), until=end_time.strftime('%Y-%m-%d'), \\\n",
    "                #                         is_happy=False, contestant=contestant)\n",
    "                #soup = BeautifulSoup(sad_time, \"html.parser\")\n",
    "                #sad_tweets = len(soup.find_all(\"p\", attrs={\"class\": \"TweetTextSize\"}))\n",
    "\n",
    "                print run_date.strftime('%Y-%m-%d')\n",
    "\n",
    "                #Save the results to a dictionary\n",
    "                ep_dict.append({run_date.strftime('%Y-%m-%d'):each_tweet})\n",
    "            result_dict[contestant] = ep_dict\n",
    "\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run scraping code individually for each season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melissa\n",
      "2009-01-05\n",
      "2009-01-12\n",
      "2009-01-19\n",
      "2009-01-26\n",
      "2009-02-02\n",
      "2009-02-09\n",
      "2009-02-16\n",
      "Molly\n",
      "2009-01-05\n",
      "2009-01-12\n",
      "2009-01-19\n",
      "2009-01-26\n",
      "2009-02-02\n",
      "2009-02-09\n",
      "2009-02-16\n",
      "Jillian\n",
      "2009-01-05\n",
      "2009-01-12\n",
      "2009-01-19\n",
      "2009-01-26\n",
      "2009-02-02\n",
      "2009-02-09\n",
      "Naomi\n",
      "2009-01-05\n",
      "2009-01-12\n",
      "2009-01-19\n",
      "2009-01-26\n",
      "2009-02-02\n",
      "Stephanie\n",
      "2009-01-05\n",
      "2009-01-12\n",
      "2009-01-19\n",
      "2009-01-26\n",
      "Lauren\n",
      "2009-01-05\n",
      "2009-01-12\n",
      "2009-01-19\n",
      "Megan\n",
      "2009-01-05\n",
      "2009-01-12\n",
      "2009-01-19\n",
      "Shannon\n",
      "2009-01-05\n",
      "2009-01-12\n",
      "2009-01-19\n",
      "Nicole\n",
      "2009-01-05\n",
      "2009-01-12\n",
      "2009-01-19\n",
      "Erica\n",
      "2009-01-05\n",
      "2009-01-12\n",
      "Kari\n",
      "2009-01-05\n",
      "2009-01-12\n",
      "Natalie\n",
      "2009-01-05\n",
      "2009-01-12\n",
      "Raquel\n",
      "2009-01-05\n",
      "Sharon\n",
      "2009-01-05\n",
      "Lisa\n",
      "2009-01-05\n",
      "Ann\n",
      "Dominique\n",
      "Emily\n",
      "Jackie\n",
      "Julie\n",
      "Nicole\n",
      "Renee\n",
      "Shelby\n",
      "Stacia\n",
      "Treasure\n"
     ]
    }
   ],
   "source": [
    "tweets13 = scrape_season_tweets(13)\n",
    "with open('tweets13.json', 'w') as fp:\n",
    "    json.dump(tweets13, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vienna\n",
      "2010-01-04\n",
      "2010-01-11\n",
      "2010-01-18\n",
      "2010-01-25\n",
      "2010-02-01\n",
      "2010-02-08\n",
      "2010-02-15\n",
      "Tenley\n",
      "2010-01-04\n",
      "2010-01-11\n",
      "2010-01-18\n",
      "2010-01-25\n",
      "2010-02-01\n",
      "2010-02-08\n",
      "2010-02-15\n",
      "Gia\n",
      "2010-01-04\n",
      "2010-01-11\n",
      "2010-01-18\n",
      "2010-01-25\n",
      "2010-02-01\n",
      "2010-02-08\n",
      "Ali\n",
      "2010-01-04\n",
      "2010-01-11\n",
      "2010-01-18\n",
      "2010-01-25\n",
      "2010-02-01\n",
      "Corrie\n",
      "2010-01-04\n",
      "2010-01-11\n",
      "2010-01-18\n",
      "2010-01-25\n",
      "Ashleigh\n",
      "2010-01-04\n",
      "2010-01-11\n",
      "2010-01-18\n",
      "Jessie\n",
      "2010-01-04\n",
      "2010-01-11\n",
      "2010-01-18\n",
      "Kathryn\n",
      "2010-01-04\n",
      "2010-01-11\n",
      "2010-01-18\n",
      "Ella\n",
      "2010-01-04\n",
      "2010-01-11\n",
      "2010-01-18\n",
      "Elizabeth\n",
      "2010-01-04\n",
      "2010-01-11\n",
      "Valishia\n",
      "2010-01-04\n",
      "2010-01-11\n",
      "Michelle\n",
      "2010-01-04\n",
      "2010-01-11\n",
      "Ashley\n",
      "2010-01-04\n",
      "Christina\n",
      "2010-01-04\n",
      "Rozlyn\n",
      "2010-01-04\n",
      "Alexa\n",
      "Caitlyn\n",
      "Channy\n",
      "Elizabeth\n",
      "Emily\n",
      "Kimberly\n",
      "Kirsten\n",
      "Sheila\n",
      "Stephanie\n",
      "Tiana\n"
     ]
    }
   ],
   "source": [
    "tweets14 = scrape_season_tweets(14)\n",
    "with open('tweets14.json', 'w') as fp:\n",
    "    json.dump(tweets14, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emily\n",
      "2011-01-03\n",
      "2011-01-10\n",
      "2011-01-17\n",
      "2011-01-24\n",
      "2011-01-31\n",
      "2011-02-07\n",
      "2011-02-14\n",
      "2011-02-21\n",
      "2011-02-28\n",
      "Chantal\n",
      "2011-01-03\n",
      "2011-01-10\n",
      "2011-01-17\n",
      "2011-01-24\n",
      "2011-01-31\n",
      "2011-02-07\n",
      "2011-02-14\n",
      "2011-02-21\n",
      "2011-02-28\n",
      "Ashley\n",
      "2011-01-03\n",
      "2011-01-10\n",
      "2011-01-17\n",
      "2011-01-24\n",
      "2011-01-31\n",
      "2011-02-07\n",
      "2011-02-14\n",
      "2011-02-21\n",
      "Shawntel\n",
      "2011-01-03\n",
      "2011-01-10\n",
      "2011-01-17\n",
      "2011-01-24\n",
      "2011-01-31\n",
      "2011-02-07\n",
      "2011-02-14\n",
      "Michelle\n",
      "2011-01-03\n",
      "2011-01-10\n",
      "2011-01-17\n",
      "2011-01-24\n",
      "2011-01-31\n",
      "2011-02-07\n",
      "Britt\n",
      "2011-01-03\n",
      "2011-01-10\n",
      "2011-01-17\n",
      "2011-01-24\n",
      "2011-01-31\n",
      "2011-02-07\n",
      "Jackie\n",
      "2011-01-03\n",
      "2011-01-10\n",
      "2011-01-17\n",
      "2011-01-24\n",
      "2011-01-31\n",
      "Alli\n",
      "2011-01-03\n",
      "2011-01-10\n",
      "2011-01-17\n",
      "2011-01-24\n",
      "2011-01-31\n",
      "Lisa\n",
      "2011-01-03\n",
      "2011-01-10\n",
      "2011-01-17\n",
      "2011-01-24\n",
      "Marissa\n",
      "2011-01-03\n",
      "2011-01-10\n",
      "2011-01-17\n",
      "2011-01-24\n",
      "Ashley\n",
      "2011-01-03\n",
      "2011-01-10\n",
      "2011-01-17\n",
      "2011-01-24\n",
      "Lindsay\n",
      "2011-01-03\n",
      "2011-01-10\n",
      "2011-01-17\n",
      "Meghan\n",
      "2011-01-03\n",
      "2011-01-10\n",
      "2011-01-17\n",
      "Stacey\n",
      "2011-01-03\n",
      "2011-01-10\n",
      "2011-01-17\n",
      "Kimberly\n",
      "2011-01-03\n",
      "2011-01-10\n",
      "Sarah\n",
      "2011-01-03\n",
      "2011-01-10\n",
      "Madison\n",
      "2011-01-03\n",
      "2011-01-10\n",
      "Keltie\n",
      "2011-01-03\n",
      "Melissa\n",
      "2011-01-03\n",
      "Raichel\n",
      "2011-01-03\n",
      "Britnee\n",
      "Cristy\n",
      "Jessica\n",
      "Jill\n",
      "Lacey\n",
      "Lauren\n",
      "Lisa\n",
      "Rebecca\n",
      "Renee\n",
      "Sarah\n"
     ]
    }
   ],
   "source": [
    "tweets15 = scrape_season_tweets(15)\n",
    "with open('tweets15.json', 'w') as fp:\n",
    "    json.dump(tweets15, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Courtney\n",
      "2012-01-02\n",
      "2012-01-09\n",
      "2012-01-16\n",
      "2012-01-23\n",
      "2012-01-30\n",
      "2012-02-06\n",
      "2012-02-13\n",
      "2012-02-20\n",
      "2012-02-27\n",
      "Lindzi\n",
      "2012-01-02\n",
      "2012-01-09\n",
      "2012-01-16\n",
      "2012-01-23\n",
      "2012-01-30\n",
      "2012-02-06\n",
      "2012-02-13\n",
      "2012-02-20\n",
      "2012-02-27\n",
      "Nicki\n",
      "2012-01-02\n",
      "2012-01-09\n",
      "2012-01-16\n",
      "2012-01-23\n",
      "2012-01-30\n",
      "2012-02-06\n",
      "2012-02-13\n",
      "2012-02-20\n",
      "Kacie\n",
      "2012-01-02\n",
      "2012-01-09\n",
      "2012-01-16\n",
      "2012-01-23\n",
      "2012-01-30\n",
      "2012-02-06\n",
      "2012-02-13\n",
      "Emily\n",
      "2012-01-02\n",
      "2012-01-09\n",
      "2012-01-16\n",
      "2012-01-23\n",
      "2012-01-30\n",
      "2012-02-06\n",
      "Rachel\n",
      "2012-01-02\n",
      "2012-01-09\n",
      "2012-01-16\n",
      "2012-01-23\n",
      "2012-01-30\n",
      "2012-02-06\n",
      "Jamie\n",
      "2012-01-02\n",
      "2012-01-09\n",
      "2012-01-16\n",
      "2012-01-23\n",
      "2012-01-30\n",
      "Casey\n",
      "2012-01-02\n",
      "2012-01-09\n",
      "2012-01-16\n",
      "2012-01-23\n",
      "2012-01-30\n",
      "Blakeley\n",
      "2012-01-02\n",
      "2012-01-09\n",
      "2012-01-16\n",
      "2012-01-23\n",
      "2012-01-30\n",
      "Jennifer\n",
      "2012-01-02\n",
      "2012-01-09\n",
      "2012-01-16\n",
      "2012-01-23\n",
      "Elyse\n",
      "2012-01-02\n",
      "2012-01-09\n",
      "2012-01-16\n",
      "2012-01-23\n",
      "Monica\n",
      "2012-01-02\n",
      "2012-01-09\n",
      "2012-01-16\n",
      "Samantha\n",
      "2012-01-02\n",
      "2012-01-09\n",
      "2012-01-16\n",
      "Jaclyn\n",
      "2012-01-02\n",
      "2012-01-09\n",
      "Erika\n",
      "2012-01-02\n",
      "2012-01-09\n",
      "Brittney\n",
      "2012-01-02\n",
      "2012-01-09\n",
      "Shawn\n",
      "2012-01-02\n",
      "Jenna\n",
      "2012-01-02\n",
      "Amber\n",
      "Amber\n",
      "Anna\n",
      "Dianna\n",
      "Holly\n",
      "Lyndsie\n",
      "Shira\n"
     ]
    }
   ],
   "source": [
    "tweets16 = scrape_season_tweets(16)\n",
    "with open('tweets16.json', 'w') as fp:\n",
    "    json.dump(tweets16, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catherine\n",
      "2013-01-07\n",
      "2013-01-14\n",
      "2013-01-21\n",
      "2013-01-28\n",
      "2013-02-04\n",
      "2013-02-05\n",
      "2013-02-11\n",
      "2013-02-18\n",
      "Lindsay\n",
      "2013-01-07\n",
      "2013-01-14\n",
      "2013-01-21\n",
      "2013-01-28\n",
      "2013-02-04\n",
      "2013-02-05\n",
      "2013-02-11\n",
      "2013-02-18\n",
      "AshLee\n",
      "2013-01-07\n",
      "2013-01-14\n",
      "2013-01-21\n",
      "2013-01-28\n",
      "2013-02-04\n",
      "2013-02-05\n",
      "2013-02-11\n",
      "2013-02-18\n",
      "Desiree\n",
      "2013-01-07\n",
      "2013-01-14\n",
      "2013-01-21\n",
      "2013-01-28\n",
      "2013-02-04\n",
      "2013-02-05\n",
      "2013-02-11\n",
      "Lesley\n",
      "2013-01-07\n",
      "2013-01-14\n",
      "2013-01-21\n",
      "2013-01-28\n",
      "2013-02-04\n",
      "2013-02-05\n",
      "Tierra\n",
      "2013-01-07\n",
      "2013-01-14\n",
      "2013-01-21\n",
      "2013-01-28\n",
      "2013-02-04\n",
      "2013-02-05\n",
      "Daniella\n",
      "2013-01-07\n",
      "2013-01-14\n",
      "2013-01-21\n",
      "2013-01-28\n",
      "2013-02-04\n",
      "Selma\n",
      "2013-01-07\n",
      "2013-01-14\n",
      "2013-01-21\n",
      "2013-01-28\n",
      "2013-02-04\n",
      "Sarah\n",
      "2013-01-07\n",
      "2013-01-14\n",
      "2013-01-21\n",
      "2013-01-28\n",
      "2013-02-04\n",
      "Robyn\n",
      "2013-01-07\n",
      "2013-01-14\n",
      "2013-01-21\n",
      "2013-01-28\n",
      "Jackie\n",
      "2013-01-07\n",
      "2013-01-14\n",
      "2013-01-21\n",
      "2013-01-28\n",
      "Amanda\n",
      "2013-01-07\n",
      "2013-01-14\n",
      "2013-01-21\n",
      "Leslie\n",
      "2013-01-07\n",
      "2013-01-14\n",
      "2013-01-21\n",
      "Kristy\n",
      "2013-01-07\n",
      "2013-01-14\n",
      "Taryn\n",
      "2013-01-07\n",
      "2013-01-14\n",
      "Kacie\n",
      "2013-01-07\n",
      "2013-01-14\n",
      "Brooke\n",
      "2013-01-07\n",
      "Diana\n",
      "2013-01-07\n",
      "Katie\n",
      "2013-01-07\n",
      "Ashley\n",
      "Ashley\n",
      "Kelly\n",
      "Keriann\n",
      "Lacey\n",
      "Lauren\n",
      "Paige\n"
     ]
    }
   ],
   "source": [
    "tweets17 = scrape_season_tweets(17)\n",
    "with open('tweets17.json', 'w') as fp:\n",
    "    json.dump(tweets17, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nikki\n",
      "2014-01-06\n",
      "2014-01-13\n",
      "2014-01-20\n",
      "2014-01-27\n",
      "2014-02-03\n",
      "2014-02-10\n",
      "2014-02-17\n",
      "2014-02-24\n",
      "Clare\n",
      "2014-01-06\n",
      "2014-01-13\n",
      "2014-01-20\n",
      "2014-01-27\n",
      "2014-02-03\n",
      "2014-02-10\n",
      "2014-02-17\n",
      "2014-02-24\n",
      "Andi\n",
      "2014-01-06\n",
      "2014-01-13\n",
      "2014-01-20\n",
      "2014-01-27\n",
      "2014-02-03\n",
      "2014-02-10\n",
      "2014-02-17\n",
      "2014-02-24\n",
      "Renee\n",
      "2014-01-06\n",
      "2014-01-13\n",
      "2014-01-20\n",
      "2014-01-27\n",
      "2014-02-03\n",
      "2014-02-10\n",
      "2014-02-17\n",
      "Chelsie\n",
      "2014-01-06\n",
      "2014-01-13\n",
      "2014-01-20\n",
      "2014-01-27\n",
      "2014-02-03\n",
      "2014-02-10\n",
      "Sharleen\n",
      "2014-01-06\n",
      "2014-01-13\n",
      "2014-01-20\n",
      "2014-01-27\n",
      "2014-02-03\n",
      "2014-02-10\n",
      "Kat\n",
      "2014-01-06\n",
      "2014-01-13\n",
      "2014-01-20\n",
      "2014-01-27\n",
      "2014-02-03\n",
      "Cassandra\n",
      "2014-01-06\n",
      "2014-01-13\n",
      "2014-01-20\n",
      "2014-01-27\n",
      "2014-02-03\n",
      "Alli\n",
      "2014-01-06\n",
      "2014-01-13\n",
      "2014-01-20\n",
      "2014-01-27\n",
      "Danielle\n",
      "2014-01-06\n",
      "2014-01-13\n",
      "2014-01-20\n",
      "2014-01-27\n",
      "Kelly\n",
      "2014-01-06\n",
      "2014-01-13\n",
      "2014-01-20\n",
      "2014-01-27\n",
      "Elise\n",
      "2014-01-06\n",
      "2014-01-13\n",
      "2014-01-20\n",
      "Lauren\n",
      "2014-01-06\n",
      "2014-01-13\n",
      "2014-01-20\n",
      "Christy\n",
      "2014-01-06\n",
      "2014-01-13\n",
      "Lucy\n",
      "2014-01-06\n",
      "2014-01-13\n",
      "Amy\n",
      "2014-01-06\n",
      "Chantel\n",
      "2014-01-06\n",
      "Victoria\n",
      "2014-01-06\n",
      "Alexis\n",
      "Amy\n",
      "Ashley\n",
      "Christine\n",
      "Kylie\n",
      "Lacy\n",
      "Lauren\n",
      "Maggie\n",
      "Valerie\n"
     ]
    }
   ],
   "source": [
    "tweets18 = scrape_season_tweets(18)\n",
    "with open('tweets18.json', 'w') as fp:\n",
    "    json.dump(tweets18, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitney\n",
      "2015-01-05\n",
      "2015-01-12\n",
      "2015-01-19\n",
      "2015-01-26\n",
      "2015-02-02\n",
      "2015-02-09\n",
      "2015-02-15\n",
      "2015-02-16\n",
      "Becca\n",
      "2015-01-05\n",
      "2015-01-12\n",
      "2015-01-19\n",
      "2015-01-26\n",
      "2015-02-02\n",
      "2015-02-09\n",
      "2015-02-15\n",
      "2015-02-16\n",
      "Kaitlyn\n",
      "2015-01-05\n",
      "2015-01-12\n",
      "2015-01-19\n",
      "2015-01-26\n",
      "2015-02-02\n",
      "2015-02-09\n",
      "2015-02-15\n",
      "2015-02-16\n",
      "Jade\n",
      "2015-01-05\n",
      "2015-01-12\n",
      "2015-01-19\n",
      "2015-01-26\n",
      "2015-02-02\n",
      "2015-02-09\n",
      "2015-02-15\n",
      "Carly\n",
      "2015-01-05\n",
      "2015-01-12\n",
      "2015-01-19\n",
      "2015-01-26\n",
      "2015-02-02\n",
      "2015-02-09\n",
      "Britt\n",
      "2015-01-05\n",
      "2015-01-12\n",
      "2015-01-19\n",
      "2015-01-26\n",
      "2015-02-02\n",
      "2015-02-09\n",
      "Megan\n",
      "2015-01-05\n",
      "2015-01-12\n",
      "2015-01-19\n",
      "2015-01-26\n",
      "2015-02-02\n",
      "Kelsey\n",
      "2015-01-05\n",
      "2015-01-12\n",
      "2015-01-19\n",
      "2015-01-26\n",
      "2015-02-02\n",
      "Ashley\n",
      "2015-01-05\n",
      "2015-01-12\n",
      "2015-01-19\n",
      "2015-01-26\n",
      "2015-02-02\n",
      "Mackenzie\n",
      "2015-01-05\n",
      "2015-01-12\n",
      "2015-01-19\n",
      "2015-01-26\n",
      "Samantha\n",
      "2015-01-05\n",
      "2015-01-12\n",
      "2015-01-19\n",
      "2015-01-26\n",
      "Ashley\n",
      "2015-01-05\n",
      "2015-01-12\n",
      "2015-01-19\n",
      "Juelia\n",
      "2015-01-05\n",
      "2015-01-12\n",
      "2015-01-19\n",
      "Nikki\n",
      "2015-01-05\n",
      "2015-01-12\n",
      "2015-01-19\n",
      "Jillian\n",
      "2015-01-05\n",
      "2015-01-12\n",
      "2015-01-19\n",
      "Amber\n",
      "2015-01-05\n",
      "2015-01-12\n",
      "Tracy\n",
      "2015-01-05\n",
      "2015-01-12\n",
      "Trina\n",
      "2015-01-05\n",
      "2015-01-12\n",
      "Alissa\n",
      "2015-01-05\n",
      "Jordan\n",
      "2015-01-05\n",
      "Kimberly\n",
      "Tandra\n",
      "2015-01-05\n",
      "Tara\n",
      "2015-01-05\n",
      "Amanda\n",
      "Bo\n",
      "Brittany\n",
      "Kara\n",
      "Michelle\n",
      "Nicole\n"
     ]
    }
   ],
   "source": [
    "tweets19 = scrape_season_tweets(19)\n",
    "with open('tweets19.json', 'w') as fp:\n",
    "    json.dump(tweets19, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shayne\n",
      "2008-03-17\n",
      "2008-03-23\n",
      "2008-03-31\n",
      "2008-04-07\n",
      "2008-04-14\n",
      "2008-04-21\n",
      "2008-04-28\n",
      "Chelsea\n",
      "2008-03-17\n",
      "2008-03-23\n",
      "2008-03-31\n",
      "2008-04-07\n",
      "2008-04-14\n",
      "2008-04-21\n",
      "2008-04-28\n",
      "Amanda\n",
      "2008-03-17\n",
      "2008-03-23\n",
      "2008-03-31\n",
      "2008-04-07\n",
      "2008-04-14\n",
      "2008-04-21\n",
      "Noelle\n",
      "2008-03-17\n",
      "2008-03-23\n",
      "2008-03-31\n",
      "2008-04-07\n",
      "2008-04-14\n",
      "Marshana\n",
      "2008-03-17\n",
      "2008-03-23\n",
      "2008-03-31\n",
      "2008-04-07\n",
      "Robin\n",
      "2008-03-17\n",
      "2008-03-23\n",
      "2008-03-31\n",
      "2008-04-07\n",
      "Ashlee\n",
      "2008-03-17\n",
      "2008-03-23\n",
      "2008-03-31\n",
      "Kelly\n",
      "2008-03-17\n",
      "2008-03-23\n",
      "2008-03-31\n",
      "Holly\n",
      "2008-03-17\n",
      "2008-03-23\n",
      "2008-03-31\n",
      "Erin\n",
      "2008-03-17\n",
      "2008-03-23\n",
      "Amy\n",
      "2008-03-17\n",
      "2008-03-23\n",
      "Kristine\n",
      "2008-03-17\n",
      "2008-03-23\n",
      "Michelle\n",
      "2008-03-17\n",
      "Carri\n",
      "2008-03-17\n",
      "Erin\n",
      "2008-03-17\n",
      "Alyssa\n",
      "Amanda\n",
      "Denise\n",
      "Devon\n",
      "Lesley\n",
      "Michele\n",
      "Rebecca\n",
      "Stacey\n",
      "Tamara\n",
      "Tiffany\n"
     ]
    }
   ],
   "source": [
    "tweets12 = scrape_season_tweets(12)\n",
    "with open('tweets12.json', 'w') as fp:\n",
    "    json.dump(tweets12, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
