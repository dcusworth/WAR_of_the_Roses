{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#*The Bachelor*: Image Processing and Analytics\n",
    "Cusworth, Heal, Leinicke, Rodriguez."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain (scrape) photos of most contestants from Seasons 13-19 of *The Bachelor*. We register (align) the grayscale images, crop them to a small region containing only the face and face-framing hair, then perform PCA on those cropped images. We then examine the contestants' varying levels of success as represented by the projection of the images onto the first two principal components. We see that in this PC-1&2 space, the 'winners' and 'runners up' classes can be separated by a linear classifier. \n",
    "\n",
    "Due to the high dimensionality of this data set, we would give this notebook about **2 hours** to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1. Scrape Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "\n",
    "from pyquery import PyQuery as pq\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This 'sirlinksalot' seemed like a really promising place to find photos. It pointed us in the direction of other good sites too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "linkVec = []\n",
    "for thisPage in [3]:#[2,3,4,5]:\n",
    "    allLinkPage=requests.get(\"http://www.sirlinksalot.net/archives/thebachelor%d.html\" % (thisPage))\n",
    "    soup = BeautifulSoup(allLinkPage.text, \"html.parser\")\n",
    "    rows = soup.find_all(\"a\")\n",
    "    for row in rows:\n",
    "        rowString = str(row)\n",
    "        checkASCII = [ord(letter) for letter in rowString]\n",
    "        if max(checkASCII)<127 and min(checkASCII)>9:\n",
    "            thisLink = str(row.get(\"href\"))\n",
    "            if \"sirlinksalot\" not in thisLink: #remove internal links\n",
    "                if \"clickbank.net\" not in thisLink: #remove ad links\n",
    "                    if \"pub43\" not in thisLink: #remove ad links\n",
    "                        if \"bilbo\" not in thisLink: #remove ad links\n",
    "                            if \"fastclick\" not in thisLink: #remove ad links\n",
    "                                if \"casalemedia\" not in thisLink: #remove ad links\n",
    "                                    linkVec.append(thisLink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1753\n"
     ]
    }
   ],
   "source": [
    "print len(linkVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sort our vector to group similar website roots\n",
    "linkVec = sorted(linkVec[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "archivedLinkVec = []\n",
    "count = 0;\n",
    "length = len(linkVec)\n",
    "for thisLink in linkVec:\n",
    "    allLinkPage=requests.get(\"https://web.archive.org/web/*/\" + thisLink)\n",
    "    soup = BeautifulSoup(allLinkPage.text, \"html.parser\")\n",
    "    if soup.find_all(\"div\", attrs={\"class\": \"date captures\"}):\n",
    "        row = soup.find_all(\"div\", attrs={\"class\": \"date captures\"})[0]\n",
    "        row = row.find(\"a\").get(\"href\")\n",
    "        archivedLinkVec.append(str(\"https://web.archive.org\" + row))\n",
    "    count = count+1\n",
    "    #print count, \" out of \", length, \" done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#we can assume that each article from the Examiner website will have similar HTML formatting. \n",
    "#So let us also note the root page.\n",
    "rootPage = []\n",
    "for link in archivedLinkVec:\n",
    "    first = 50#link.find(\"www\") \n",
    "    last = link.find(\".com\")+4\n",
    "    rootPage.append(link[first:last])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#collect into a data frame\n",
    "linkDF = pd.DataFrame()\n",
    "linkDF['archived_link']=archivedLinkVec\n",
    "linkDF['root_page']= rootPage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myGroups = linkDF.groupby('root_page')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['blog.zap2it.com' 'insidetv.ew.com' '' 'www.digitalspy.com'\n",
      " 'www.eonline.com' 'www.examiner.com' 'www.people.com' 'www.usmagazine.com']\n",
      "There are 7 unique groups.\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for group in linkDF.root_page.unique():\n",
    "    if group:\n",
    "        count = count+1\n",
    "print linkDF.root_page.unique()\n",
    "print \"There are\",count, \"unique groups.\" #this shows that the number of unique websites \n",
    "                                          #whose formatting we need to learn is pretty limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 25)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SEASON 13 BLUE BACKGROUND\n",
    "season13Names =[]\n",
    "season13PicLinks=[]\n",
    "season13Link = \"http://www.buddytv.com/slideshows/meet-the-cast-of-the-bachelor-season-13.aspx\"\n",
    "testPage=requests.get(season13Link)\n",
    "soup = BeautifulSoup(testPage.text, \"html.parser\")\n",
    "scripts = soup.find_all(\"script\")\n",
    "for script in scripts:\n",
    "    if script.string:\n",
    "        if \"Slideshow.SlideshowNavigator\" in str(script.string):\n",
    "            mystring = str(script.string)\n",
    "            myvec = mystring.split(\"{\")\n",
    "            myvec = myvec[7:32]\n",
    "            for k in myvec:\n",
    "                fromhere = str(k).find(\"Title\")+8\n",
    "                tohere = str(k).find(\"Blurb\")-3\n",
    "                season13Names.append(k[fromhere:tohere])\n",
    "                fromhere = str(k).find(\"ImageId\")+9\n",
    "                tohere = str(k).find(\"PhotoCredit\")-2\n",
    "                season13PicLinks.append(\"http://images.buddytv.com/btv_2_%d_1_590_-1_0_/meet-the-cast-of--th.jpg\" % int(k[fromhere:tohere]))\n",
    "len(season13Names), len(season13PicLinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 26)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### SEASON 14 BLUE BACKGROUND\n",
    "season14Link = \"http://www.realitytea.com/2009/12/17/meet-the-25-bachelorettes-of-the-bachelor-14-on-the-wings-of-love-photos/\"\n",
    "season14PicLinks = []\n",
    "season14Names = []\n",
    "testPage=requests.get(season14Link)\n",
    "soup = BeautifulSoup(testPage.text, \"html.parser\")\n",
    "rows = soup.find_all(\"img\")\n",
    "for row in rows:\n",
    "    if row.get(\"src\"):\n",
    "        if \"the-bachelor-14\" in row.get(\"src\"):\n",
    "            season14PicLinks.append(str(row.get(\"src\")))\n",
    "            myname = str(row.get(\"alt\"))\n",
    "            myname = myname.split(\"14 \")[1]\n",
    "            season14Names.append(myname)\n",
    "len(season14Names),len(season14PicLinks)\n",
    "#season14Names = [\"Alexa Mcallister\",\"Ali Fedotowsky\",\"Ashleigh Hunt\",\"Ashley Almore\",\"Caitlyn McCabe\",\"Channy Choch\",\"Christina McCasland\",\"Corrie\",\"Elizabeth Kitt\",\"Elizabeth Kreft\",\"Ella Nolan\",\"Emily Harkins\",\"Gia Alliemand\",\"Jessie Sulidis\",\"Kathryn\",\"Kimberly\",\"Kirsten\",\"Michelle Kujawa\",\"Rozlyn Papa\",\"Sheila\",\"Stephanie\",\"Tenley\",\"Tiana\",\"Valishia Savage\",\"Vienna Girardi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SEASON 15 BLUE BACKGROUND\n",
    "season15Link = \"http://www.realitytea.com/2010/12/21/photos-meet-the-25-bachelorettes-of-brad-womacks-the-bachelor-15/\"\n",
    "season15PicLinks = []\n",
    "season15Names=[]\n",
    "testPage=requests.get(season15Link)\n",
    "soup = BeautifulSoup(testPage.text, \"html.parser\")\n",
    "rows = soup.find_all(\"img\")\n",
    "for row in rows:\n",
    "    if row.get(\"src\"):\n",
    "        if \"the-bachelor-15\" in row.get(\"src\"):\n",
    "            season15PicLinks.append(str(row.get(\"src\")))\n",
    "            season15Names.append(str(row.get(\"alt\")))\n",
    "            \n",
    "len(season15Names),len(season15PicLinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 26)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SEASON 16 BLUE BACKGROUND\n",
    "season16Link = \"http://www.realitytea.com/2011/12/12/photos-%E2%80%93-meet-the-25-bachelorettes-of-ben-flajnik%E2%80%99s-the-bachelor-season-16/\"\n",
    "season16PicLinks = []\n",
    "season16Names = []\n",
    "testPage=requests.get(season16Link)\n",
    "soup = BeautifulSoup(testPage.text, \"html.parser\")\n",
    "rows = soup.find_all(\"img\")\n",
    "for row in rows:\n",
    "    if row.get(\"data-lazy-src\"):\n",
    "        if \"The_Bachelor_16_\" in row.get(\"data-lazy-src\"):\n",
    "            season16PicLinks.append(str(row.get(\"data-lazy-src\")))\n",
    "            season16Names.append(str(row.get(\"alt\")))\n",
    "len(season16Names),len(season16PicLinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 25)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SEASON 17 BLUE BACKGROUND\n",
    "season17Link = \"http://www.realitytea.com/2012/09/28/photos-meet-the-26-bachelorettes-of-sean-lowes-the-bachelor-season-17/\"\n",
    "season17PicLinks = []\n",
    "season17Names = []\n",
    "testPage=requests.get(season17Link)\n",
    "soup = BeautifulSoup(testPage.text, \"html.parser\")\n",
    "rows = soup.find_all(\"img\")\n",
    "for row in rows:\n",
    "    if row.get(\"data-lazy-src\"):\n",
    "        if \"-sean-lowe-season\" in row.get(\"data-lazy-src\"):\n",
    "            season17PicLinks.append(str(row.get(\"data-lazy-src\")))\n",
    "            season17Names.append(str(row.get(\"title\")))\n",
    "len(season17Names),len(season17PicLinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 27)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SEASON 18 BLUE BACKGROUND\n",
    "season18Links=[\"http://www.usmagazine.com/entertainment/pictures/the-bachelor-season-18-meet-juan-pablos-bachelorettes-2013412/343%d\" % k for k in range(71,98)]\n",
    "season18PicLinks = []\n",
    "season18Names = []\n",
    "for link in season18Links:\n",
    "    testPage=requests.get(link)\n",
    "    soup = BeautifulSoup(testPage.text, \"html.parser\")\n",
    "    rows = soup.find_all(\"figure\")\n",
    "    for row in rows:\n",
    "        if row.get(\"data-slug\"):\n",
    "            if \"18-meet-juan-pablos\" in row.get(\"data-slug\"):\n",
    "                thisimg = row.find_all(\"img\")[0]\n",
    "                thisimg = thisimg.get(\"src\")\n",
    "                season18PicLinks.append(\"http:\" + thisimg)\n",
    "                season18Names.append(str(row.get(\"data-title\")))\n",
    "len(season18Names),len(season18PicLinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 30)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SEASON 19 BLUE BACKGROUND\n",
    "season19Links = \"http://www.accesshollywood.com/galleries/meet-bachelor-chris-soules-30-farm-girls-in-training-4692\"\n",
    "season19PicLinks = []\n",
    "season19Names =[]\n",
    "testPage=requests.get(season19Links)\n",
    "soup = BeautifulSoup(testPage.text, \"html.parser\")\n",
    "rows = soup.find_all(\"img\")\n",
    "for row in rows:\n",
    "    if row.get(\"alt\"):\n",
    "        if \"starring Chris Soules\" in row.get(\"alt\"):\n",
    "            season19PicLinks.append(str(row.get(\"src\")))\n",
    "            b = row.get(\"alt\")\n",
    "            season19Names.append(b.split()[0])\n",
    "len(season19Names),len(season19PicLinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(187, 187)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#AGGREGATE PICS AND NAMES\n",
    "allPicLinks = np.concatenate([season13PicLinks,season14PicLinks,season15PicLinks,season16PicLinks,season17PicLinks,season18PicLinks,season19PicLinks])\n",
    "allNames = np.concatenate([season13Names,season14Names,season15Names,season16Names,season17Names,season18Names,season19Names])\n",
    "\n",
    "len(allPicLinks),len(allNames)\n",
    "#might also be helpful: http://www.buddytv.com/tvshow/page/the-bachelor-cast-1.aspx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2. Save links to local machine & Turn into NumPy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment this block if running for the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## SAVE ALL PICS TO LOCAL MACHINE\n",
    "## only really need to do this once\n",
    "## make sure you're in the right folder on your local machine\n",
    "\n",
    "# import urllib\n",
    "# count = 0\n",
    "# for link in allPicLinks:\n",
    "#     urllib.urlretrieve(link, \"bachelorette_pics/bachelorette%d.jpg\" % count)\n",
    "#     count = count+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read from the saved file on your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187\n"
     ]
    }
   ],
   "source": [
    "picsAsMatrices = []\n",
    "badImages = [5,11,20,25,105]\n",
    "old_allNames = allNames\n",
    "print len(old_allNames)\n",
    "allNames = []\n",
    "viableIndices = np.concatenate([range(0,5),range(6,11),range(12,20),range(21,25),range(26,105),range(106,187)])\n",
    "for count in viableIndices:\n",
    "    picsAsMatrices.append(sp.misc.imread(\"bachelorette_pics/bachelorette%d.jpg\" % count))\n",
    "    allNames.append(old_allNames[count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "The _imaging C module is not installed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-896ea08b742c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#turn pics into B&W, then into (binary) numpy arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mviableIndices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mimg0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bachelorette_pics/bachelorette%d.jpg\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LA'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mimg0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"greyscale_pics/greyscale%d.png\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mimg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/PIL/Image.pyc\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, data, dither, palette, colors)\u001b[0m\n\u001b[1;32m    677\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/PIL/ImageFile.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    162\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;31m# look for read/seek overrides\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/PIL/ImageFile.pyc\u001b[0m in \u001b[0;36mload_prepare\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0;31m# create palette (optional)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"P\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/PIL/Image.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, id)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# module placeholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The _imaging C module is not installed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: The _imaging C module is not installed"
     ]
    }
   ],
   "source": [
    "picsAsBinaryMatrices = []\n",
    "#turn pics into B&W, then into (binary) numpy arrays\n",
    "for count in viableIndices:\n",
    "    img0 = Image.open(\"bachelorette_pics/bachelorette%d.jpg\" % count).convert('LA')\n",
    "    img0.save(\"greyscale_pics/greyscale%d.png\" % count)\n",
    "    img1 = np.array(img0.getdata())\n",
    "    img2 = (img1[:,1]-img1[:,0]).reshape(img0.size[1], img0.size[0])\n",
    "    picsAsBinaryMatrices.append(img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plt.imshow(picsAsBinaryMatrices[156]);\n",
    "#im1 = picsAsBinaryMatrices[15]\n",
    "#im2 = picsAsBinaryMatrices[16]\n",
    "#mat1 = np.array(im1.getdata())#.reshape(im1.size[0], im1.size[1], 2)\n",
    "#mat11= (mat1[:,1]-mat1[:,0]).reshape(im1.size[1], im1.size[0])\n",
    "#plt.imshow(mat11)\n",
    "\n",
    "#heights,widths=[],[]\n",
    "#for pic in picsAsBinaryMatrices:\n",
    "#    heights.append(pic.size[0])\n",
    "#    widths.append(pic.size[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test code. I tried to use PIL but couldn't get eigenvalues, eigenvectors, etc. with it. \n",
    "\n",
    "\n",
    "So, I made my own registration package (see next section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#cropHeight = min(heights)\n",
    "#cropWidth = min(widths)\n",
    "#cropParams = [0,0,cropHeight,cropWidth]\n",
    "\n",
    "#im1 = im1.crop(cropParams)\n",
    "#im2 = im2.crop(cropParams)\n",
    "#Image.blend(im1,im2,alpha=0.5)\n",
    "\n",
    "#newIm = picsAsBinaryMatrices[0]\n",
    "#newIm = newIm.crop(cropParams)\n",
    "#for pic in picsAsBinaryMatrices:\n",
    "#    tempPic = pic.crop(cropParams)\n",
    "#    newIm = Image.blend(newIm,tempPic,alpha=0.25)\n",
    "\n",
    "##test\n",
    "#newIm = np.add(picsAsBinaryMatrices[0],picsAsBinaryMatrices[1])\n",
    "#newIm = np.add(newIm,picsAsBinaryMatrices[4])\n",
    "#plt.imshow(newIm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3. Registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#REGISTRATION, using cross-correlation. \n",
    "from scipy import signal\n",
    "from scipy import misc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#decRate = 20\n",
    "#sig0 = [[0,0,0],[0,1,0],[0,0,0]]\n",
    "#sig1 = picsAsBinaryMatrices[0][85:135,125:175]\n",
    "#sig2 = picsAsBinaryMatrices[1][80:130,100:150]\n",
    "#decSig1 = sp.misc.imresize(sig1, decRate, interp='nearest', mode=None)\n",
    "#decSig2 = sp.misc.imresize(sig2, decRate, interp='nearest', mode=None)\n",
    "#xcorr = sp.signal.correlate2d(sig0,sig0, mode='full', boundary='symm')\n",
    "\n",
    "#print xcorr\n",
    "#print\n",
    "#print xcorr.shape\n",
    "#plt.imshow(xcorr);\n",
    "\n",
    "#plt.imshow(decSig1)\n",
    "\n",
    "#subset4= picsAsBinaryMatrices[testnum][50:(50+framesize),50:(50+framesize)]\n",
    "#subset5= picsAsBinaryMatrices[refnum][50:(50+framesize),50:(50+framesize)]\n",
    "#xcorr = sp.signal.correlate2d(picsAsBinaryMatrices[refnum],subset4, mode='same', boundary='symm')\n",
    "#print (picsAsBinaryMatrices[refnum]).shape\n",
    "\n",
    "#decimated = sp.misc.imresize(picsAsBinaryMatrices[0], 10, interp='bilinear', mode=None)\n",
    "#plt.imshow(decimated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "refnum = 6\n",
    "testnum = 17\n",
    "framesize = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mirror image borders\n",
    "def mirror_image(im):\n",
    "    im2 = np.concatenate((im[:,::-1],im),axis=1)\n",
    "    im2 = np.concatenate((im2,im[:,::-1]),axis=1)\n",
    "    mirrim = np.concatenate((im2,im2[::-1,:]),axis=0)\n",
    "    mirrim = np.concatenate((im2[::-1,:],mirrim),axis=0)\n",
    "    return mirrim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#find index of max cross-correlation\n",
    "#from http://stackoverflow.com/questions/21989513/finding-index-of-maximum-value-in-array-with-numpy\n",
    "def nanargmax(a):\n",
    "    idx = np.argmax(a, axis=None)\n",
    "    multi_idx = np.unravel_index(idx, a.shape)\n",
    "    if np.isnan(a[multi_idx]):\n",
    "        nan_count = np.sum(np.isnan(a))\n",
    "        # In numpy < 1.8 use idx = np.argsort(a, axis=None)[-nan_count-1]\n",
    "        idx = np.argpartition(a, -nan_count-1, axis=None)[-nan_count-1]\n",
    "        multi_idx = np.unravel_index(idx, a.shape)\n",
    "    return multi_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crop_pic(im,optpx): #image to align, optimal translation coordinates w.r.t. to some defined reference image\n",
    "    \n",
    "    fsz=0#110 #107 worked with ref 6 and num 4    \n",
    "    #standard image size: (400, 267)\n",
    "    px = optpx[1]-fsz\n",
    "    py = optpx[0]-fsz\n",
    "    x_std = im.shape[1]\n",
    "    y_std = im.shape[0]\n",
    "    print x_std+px,x_std*2+px\n",
    "    print y_std+py,y_std*2+py\n",
    "    aligned = mirror_image(im)[(y_std-py):(y_std*2-py),(x_std-px):(x_std*2-px)] #y, then x. it's weird I know but yes.\n",
    "    #crops image to original size about that good part. uses image mirroring.\n",
    "    \n",
    "    return aligned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Analysis goal: find eigenvalues & eigenvectors and then do PCA or something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def crop_to_min_size(im,minsz=(400,267)):\n",
    "    \n",
    "    oldszx,oldszy = im.shape\n",
    "    newszx,newszy = minsz\n",
    "\n",
    "    newx0,newy0 = 0,0\n",
    "    newxf,newyf = minsz   \n",
    "    if oldszx > newszx:\n",
    "        newx0 = 0\n",
    "        newxf = newx0 + newszx\n",
    "    if oldszy > newszy:\n",
    "        newy0 = (oldszy-newszy)/2\n",
    "        newyf = newy0 + newszy    \n",
    "    cropped = im[newx0:newxf,newy0:newyf]\n",
    "    \n",
    "    return cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crop_pic(im,optpx,fc): #image to align, optimal translation coordinates w.r.t. to some defined reference image\n",
    "    \n",
    "    px = optpx[1]-fc[1]\n",
    "    py = optpx[0]-fc[0]\n",
    "    x_std = im.shape[1]\n",
    "    y_std = im.shape[0]\n",
    "    aligned = mirror_image(im)[(y_std-py):(y_std*2-py),(x_std+px):(x_std*2+px)] #y, then x. it's weird I know but yes.\n",
    "    #crops image to original size about that good part. uses image mirroring.\n",
    "    \n",
    "    return aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def align_to_ref(imn,refn,fsz): #image number, ref number.\n",
    "\n",
    "    myim = crop_to_min_size(picsAsBinaryMatrices[imn]) #make smallest size so can add all\n",
    "    refim = picsAsBinaryMatrices[refn]\n",
    "    \n",
    "    fs = framesize; #frame size\n",
    "    fc = fs/2,np.rint((myim.shape[1])/2) #frame center\n",
    "    select_face = (fc[0]-fs/2),(fc[0]+fs/2),(fc[1]-fs/2),(fc[1]+fs/2)\n",
    "    subset= myim[select_face[0]:select_face[1],select_face[2]:select_face[3]]\n",
    "    xcorr = sp.signal.correlate2d(refim,subset, mode='same', boundary='symm') #takes time\n",
    "    minx,miny = xcorr.shape\n",
    "    \n",
    "    optpix = nanargmax(xcorr[0:miny/2][0:minx/2]) #best way to translate the picture for alignment\n",
    "    aligned_image = crop_pic(myim,optpix,fc)\n",
    "    \n",
    "    return aligned_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##All the single ladies (All the single ladies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test code. \n",
    "\n",
    "\n",
    "You can see, by running this block, that we're actually doing something with the cross-correlation; not all of the work was done heuristically in `align_to_ref`. Nice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#aligned = []\n",
    "#for i in range(146,149):\n",
    "#    aligned.append(align_to_ref(i,6,framesize))\n",
    "#\n",
    "#fig = plt.figure(figsize=(15,30))\n",
    "#for a,j,i in zip(aligned,range(1,18),range(146,149)):\n",
    "#    s=fig.add_subplot(5,4,j)\n",
    "#    s.imshow(a+crop_to_min_size(picsAsBinaryMatrices[i]))\n",
    "\n",
    "#meanpic = np.mean([crop_to_min_size(a) for a in aligned], axis = 0)\n",
    "#plt.imshow(meanpic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#5. Align and save images to local machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This took my computer about 29 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# framesize = 200\n",
    "\n",
    "# import time\n",
    "# start = time.time()\n",
    "\n",
    "# alignedSet = []\n",
    "# for thisPic in range(0,len(picsAsBinaryMatrices)):\n",
    "#     #using image 6 as a reference because she is pretty centered in the frame\n",
    "#     alignedSet.append(align_to_ref(thisPic,6,framesize))\n",
    "#     print \"Aligning image \", thisPic+1, \" out of \", len(picsAsBinaryMatrices)\n",
    "\n",
    "# end = time.time()\n",
    "# print end - start\n",
    "\n",
    "# for thisPic,count in zip(alignedSet,viableIndices):\n",
    "#     np.save(\"aligned_pics/aligned%d.npy\" % count,thisPic) #saved as numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve from local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('Users/Heal/Documents/15FClasses/Data\\Science/MR-DC-KH-Final-Project/aligned_pics')\n",
    "tomat = np.zeros((400,267),dtype=int)\n",
    "\n",
    "alignedSet = []\n",
    "for count in viableIndices:\n",
    "    tomat = np.load(\"aligned_pics/aligned%d.npy\" % count)\n",
    "    alignedSet.append(tomat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove that one, too-small group shot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alignedSetNew = alignedSet\n",
    "\n",
    "#find the groupshot\n",
    "for thisPic,count in zip(alignedSetNew,range(0,len(alignedSetNew))):\n",
    "    if thisPic.shape[0]<400:\n",
    "        #delete that groupshot\n",
    "        alignedSetNew = np.delete(alignedSetNew,count)       \n",
    "        viableIndices = np.delete(viableIndices,count)\n",
    "        allNames = np.delete(allNames,count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that the images are too high-res and large. There are too many pixels, and thus the dimension (400x267) is too high given the number of samples (182) that we have. Fortunately, we have registered the images, so we know where we can safely crop the photos. Let's try x=[25:250], y=[50:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.signal import decimate\n",
    "\n",
    "smallSet = []\n",
    "downsampled = []\n",
    "for a in alignedSetNew:\n",
    "    newSmall = a[75:200,75:200]\n",
    "    smallSet.append(newSmall)\n",
    "    downsampled.append(decimate(newSmall,10))\n",
    "print downsampled[0].shape[0]*downsampled[0].shape[1]    \n",
    "print smallSet[0].shape[0]*smallSet[0].shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't end up downsampling after all. *but we could have!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#6. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the average of all the faces. Not too descriptive, since we have so many images and our alignment wasn't perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alignedMean = np.mean(smallSet,axis=0); #zoom in on the face\n",
    "plt.imshow(alignedMean);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alignedMean = np.mean(alignedSet,axis=0); #overall image\n",
    "plt.imshow(alignedMean);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Hi, I'm a terrifying demon face\" -Average of the matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alignedSetNew=smallSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dims = alignedSetNew[0].shape\n",
    "vecLen = dims[0]*dims[1]\n",
    "matsAsVects=[]\n",
    "for im in alignedSetNew:\n",
    "    matsAsVects.append(np.reshape(im,vecLen))\n",
    "#now matsAsVects is a vector of 182 of these 106800-vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $i$th ROW of the following matrix $X$ corresponds to the flattened matrix `alignedSet[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X = np.concatenate(np.transpose([matsAsVects]),axis=1)\n",
    "Y = np.transpose(np.concatenate(np.transpose([smallSet])))\n",
    "X = Y[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Bootstrappin'.\n",
    "numSamps = X.shape[0]\n",
    "indboot = np.random.choice(range(0,numSamps),vecLen-numSamps)\n",
    "Xboot = []\n",
    "count = 0\n",
    "for i in indboot:\n",
    "    Xboot.append(X[i])\n",
    "    count = count +1\n",
    "Xboot = np.concatenate([X,Xboot])\n",
    "Xboot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from Lab 5\n",
    "\n",
    "# miniblocks =[]\n",
    "# testsize =10000\n",
    "# for x in Xboot[0:(testsize^2)]:\n",
    "#     miniblocks.append(x[0:testsize][0:testsize])\n",
    "    \n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First principal component of a winner? So we can have a side-by-side comparison.\n",
    "\n",
    "WHAT THE WHAT this thing took an **hour and a half**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import time\n",
    "#start = time.time()\n",
    "#Xvec = pca.fit_transform(Xboot)\n",
    "#end = time.time()\n",
    "#print \"That took \" ,end - start, \" seconds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.save(\"pca_copy\",pca) #ok let's save this so we never have to run it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca.components_[0] #should be equal to the mean of the faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from Lab 5\n",
    "testsize = 15625\n",
    "def normit(a):\n",
    "    a=(a - a.min())/(a.max() -a.min())\n",
    "    a=a*256\n",
    "    return np.round(a)\n",
    "def getNC(pc, j):\n",
    "    size=testsize*testsize\n",
    "    r=pc.components_[j]#[0:size:3]\n",
    "    r=normit(r)\n",
    "    return r\n",
    "def display_component(pc, j):\n",
    "    r = getNC(pc,j)\n",
    "    r = np.array(r)\n",
    "    myArray = np.zeros((np.sqrt(testsize)), 'uint8')\n",
    "    myArray = np.transpose(np.reshape(r,(np.sqrt(testsize),np.sqrt(testsize))))\n",
    "    plt.imshow(myArray)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def display_pc(pc, j,ax):\n",
    "    r = getNC(pc,j)\n",
    "    r = np.array(r)\n",
    "    myArray = np.zeros((np.sqrt(testsize)), 'uint8')\n",
    "    myArray = np.transpose(np.reshape(r,(np.sqrt(testsize),np.sqrt(testsize))))\n",
    "    ax.imshow(myArray)\n",
    "    \n",
    "#display_component(pca,2)\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "count = 0\n",
    "for r in range(0,10):\n",
    "    s=fig.add_subplot(2,5,r+1)\n",
    "    s.set_title(\"PC %d\" % r)\n",
    "    s.set_xlabel(\"Frac. Var. Expl.: %f\" % pca.explained_variance_ratio_[r] )\n",
    "    s.set_xticks([])\n",
    "    s.set_yticks([])\n",
    "    display_pc(pca,r,s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximately 70% of the variance can be explained by these top 10 principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(Xvec[0:182,i]),len(allNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##season winners\n",
    "win_ind = [46,52,81,106,129,181]\n",
    "\n",
    "##first runners up\n",
    "run_ind = [10,43,50,92,119,140,157]\n",
    "\n",
    "##eliminated round 1\n",
    "season13elim = [0,1,2,4,5,13,16,18,19,21]\n",
    "season14elim = [22,26,27,31,33,37,38,41,42,44]\n",
    "season15elim = [48,51,53,55,58,59,62,69,70,71]\n",
    "season16elim = [75,77,82,86,93,99]\n",
    "season17elim = [103,104,111,113,115,116]\n",
    "season18elim = [151,149,146,142,135,134,133,130,126]\n",
    "season19elim = [153,158,160,167,169,172,173,175]\n",
    "\n",
    "elim_ind = np.concatenate([season13elim,season14elim,season15elim,season16elim,season17elim,season18elim,season19elim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##AFTER the PCA process, we add success labels\n",
    "winners = np.zeros((len(allNames),)) # 0: all others\n",
    "winners[win_ind] = [1,1,1,1,1,1] #1: season winners\n",
    "winners[run_ind] = [.5,.5,.5,.5,.5,.5,.5] #0.5: runners up\n",
    "winners[elim_ind] = [-1,-1,-1,-1,-1,-1,-1] #-1: elim first week\n",
    "\n",
    "df = pd.DataFrame({\"name\":allNames,\"winner\":winners})\n",
    "for i in range(pca.explained_variance_ratio_.shape[0]):\n",
    "    df[\"pc%i\" % (i+1)] = Xvec[0:182,i]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "\n",
    "c0=sns.color_palette()[0]\n",
    "c1=sns.color_palette()[1]\n",
    "c2=sns.color_palette()[2]\n",
    "c3=sns.color_palette()[3]\n",
    "c4=sns.color_palette()[4]\n",
    "\n",
    "colors = [c1,c0,c4,c2]\n",
    "for label, color, transp in zip(df['winner'].unique(), colors,[0.9,0.9,0.9,0.9]):\n",
    "    mask = df['winner']==label\n",
    "    plt.scatter(df[mask]['pc1'], df[mask]['pc2'], c=color, label=label,s=400,alpha=transp);\n",
    "plt.legend([\"Eliminated Round 1\",\"Eliminated Round 2+\",\"Runners Up\",\"Season Winners\"]);\n",
    "plt.title(\"Relationship Between Principal Components and Success in 'The Bachelor'\")\n",
    "plt.xlabel(\"First Principal Component\");\n",
    "plt.ylabel(\"Second Principal Component\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoa! Linearly separable RunnersUp/Winners!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##plot our winners/losers\n",
    "\n",
    "fig1 = plt.figure(figsize=(40,30))\n",
    "fig1.suptitle(\"Season Winners\",fontsize=40)\n",
    "count = 0\n",
    "for a in win_ind:\n",
    "    count = count+1\n",
    "    s=fig1.add_subplot(10,10,count)\n",
    "    s.imshow(alignedSetNew[int(a)])\n",
    "    s.set_xticks([])\n",
    "    s.set_yticks([])\n",
    "    \n",
    "fig2 = plt.figure(figsize=(40,30))\n",
    "fig2.suptitle(\"First Runners Up\",fontsize=40)\n",
    "count = 0\n",
    "for a in run_ind:\n",
    "    count = count+1\n",
    "    s=fig2.add_subplot(10,10,count)\n",
    "    s.imshow(alignedSetNew[int(a)])\n",
    "    s.set_xticks([])\n",
    "    s.set_yticks([])\n",
    "    \n",
    "fig3 = plt.figure(figsize=(20,20))\n",
    "fig3.suptitle(\"Eliminated Round 1\",fontsize=40)\n",
    "count = 0\n",
    "for a in elim_ind:\n",
    "    count = count+1\n",
    "    s=fig3.add_subplot(10,10,count)\n",
    "    s.imshow(alignedSetNew[int(a)])\n",
    "    s.set_xticks([])\n",
    "    s.set_yticks([])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the alignment wasn't perfect. Given more time, we could do much better. But, when comparing to the pre-aligned images, the alignment **did** help immensely. In terms of the alignment, given more time we would like to explore rotations and fixed-ratio scaling. \n",
    "\n",
    "Now we put everything in a nice dictionary so that we can use it with our other analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##just checking stuff\n",
    "#print [df['name'][j] for j in win_ind]\n",
    "#[(i, allNames[i]) for i in range(0,len(allNames))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "seasons=[13,14,15,16,17,18,19]\n",
    "# allDict = pd.DataFrame({\"Season\":seasons})\n",
    "# print allDict.head() \n",
    "\n",
    "##these indices corresp. to the list above, AFTER, viableindices has been implemented.\n",
    "s13=range(0,22)\n",
    "s14=range(22,47)\n",
    "s15=range(47,75)\n",
    "s16=range(75,101)\n",
    "s17=range(101,125)\n",
    "s18=range(125,152)\n",
    "s19=range(152,181)\n",
    "s = [s13,s14,s15,s16,s17,s18,s19]\n",
    "\n",
    "seasonDicts=[]\n",
    "for season,inds in zip(seasons,s):\n",
    "    seasonNames = [allNames[i] for i in inds]\n",
    "    seasonPC01 = [(df['pc1'][i],df['pc2'][i]) for i in inds]\n",
    "    \n",
    "    seasonDicts.append( dict(zip(seasonNames,seasonPC01)) )\n",
    "\n",
    "allDict =  dict(zip(seasons,seasonDicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('theBachelorPCA_dict.csv','wb') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerows(allDict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
