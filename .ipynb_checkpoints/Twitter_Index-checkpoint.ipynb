{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code takes the libraries of data scraped from twitter and converts them into predictors that will be used for the future regression. We are interested in two types of predictors from the twitter data:\n",
    "\n",
    "1) The share of tweets a contestant received for one episode.\n",
    "\n",
    "2) The general positivity of a contestant's tweets for one episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import oauth2\n",
    "import simplejson\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "from pyquery import PyQuery as pq\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import datetime\n",
    "import json\n",
    "import unittest, time, re\n",
    "from pattern.en import parse\n",
    "from pattern.en import pprint\n",
    "from pattern.vector import stem, PORTER, LEMMA\n",
    "from sklearn.feature_extraction import text\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load Contestant Name Data from wiki scrape\n",
    "with open(\"tempdata/seasonsDict.json\") as json_file:\n",
    "    wiki_data = json.load(json_file)\n",
    "\n",
    "#Fix known formatting problems:\n",
    "wiki_data['19'][19]['eliminated'] = u'Eliminated in week 2'\n",
    "wiki_data['19'][20]['eliminated'] = u'Eliminated in week 1'\n",
    "\n",
    "w19 = []\n",
    "for ww in wiki_data['19'][0:29]:\n",
    "    w19.append(ww)\n",
    "    \n",
    "wiki_data['19'] = w19\n",
    "\n",
    "#Load date guide\n",
    "date_guide = pd.read_csv(\"date_guide.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get all contestant names\n",
    "cont_nam = []\n",
    "for wkey in wiki_data.keys():\n",
    "    for person in wiki_data[wkey]:\n",
    "        cont_nam.append(person['name'])\n",
    "\n",
    "#Strip names with weird formatting\n",
    "def url_strip(r):\n",
    "    if bool(re.search(\"href\", r)):\n",
    "        oval = r.split(\">\")[1].replace(\"</a\",\"\")\n",
    "    else:\n",
    "        oval = r\n",
    "    if bool(re.search(\"u\\\"\", oval)):\n",
    "        return oval.replace(\"u\\\"\",\"\").replace(\"[\",\"\")\n",
    "    else:\n",
    "        return oval\n",
    "    \n",
    "full_names = map(url_strip, cont_nam)\n",
    "\n",
    "#Get just first names\n",
    "first_names = set(map(lambda r: r.split(\" \")[0], full_names))\n",
    "\n",
    "bach_names = ['Jason', 'Jake', 'Brad', 'Ben', 'Sean', 'Juan', 'Chris']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Create Corpus\n",
    "\n",
    "Here we create a corpus of adjectives and adverbs from our entire body of tweets. Since Twitter is not copy-edited, we have to intensely filter our results to collect the words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets12.json  is done\n",
      "tweets13.json  is done\n",
      "tweets14.json  is done\n",
      "tweets15.json  is done\n",
      "tweets16.json  is done\n",
      "tweets17.json  is done\n",
      "tweets18.json  is done\n",
      "tweets19.json  is done\n"
     ]
    }
   ],
   "source": [
    "#First load all the tweets from all seasons\n",
    "all_tweets = []\n",
    "for iseason in range(12,20):\n",
    "    file_name = \"tweets\" + str(iseason) + \".json\"\n",
    "    with open(file_name) as json_file:\n",
    "        tdat = json.load(json_file)\n",
    "    \n",
    "    for tkey in tdat.keys():\n",
    "        cont_dat = tdat[tkey]\n",
    "        if cont_dat is not None:\n",
    "            for cc in cont_dat:\n",
    "                ep_dat = cc.keys()\n",
    "                for tweet in cc[ep_dat[0]]:\n",
    "                    all_tweets.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Next flatten all the tweets into one list - where one sentence = one list entry\n",
    "\n",
    "#Get all twitter sentences over all tweets   \n",
    "#Flatten all sentences into an array\n",
    "tweet_periods = map(lambda r: r.split(\".\"), all_tweets)\n",
    "tweet_flat1 = [item for sublist in tweet_periods for item in sublist]\n",
    "\n",
    "tweet_questions = map(lambda r: r.split(\"?\"), tweet_flat1)\n",
    "tweet_flat2 = [item for sublist in tweet_questions for item in sublist]\n",
    "\n",
    "tweet_exclaim = map(lambda r: r.split(\"!\"), tweet_flat2)\n",
    "tweet_flat3 = [item for sublist in tweet_exclaim for item in sublist]\n",
    "\n",
    "#Filter out empty sentences\n",
    "tweet_sentences = filter(lambda r: r not in \"\", tweet_flat3)\n",
    "\n",
    "#Replace hypens as spaces\n",
    "tweet_sentences = map(lambda r: r.replace(\"-\",\" \"), tweet_sentences)\n",
    "tweet_sentences = map(lambda r: r.replace(\"=\",\" \"), tweet_sentences)\n",
    "\n",
    "#Strip weird characters from words\n",
    "tweet_encode = [tt.encode(\"ascii\", \"ignore\") for tt in tweet_sentences]\n",
    "tweet_process_output = map(lambda r: r.translate(None,\"*@#\\/[]()\"), tweet_encode)\n",
    "\n",
    "#Filter out strange results in our vocabulary\n",
    "good_sentences = filter(lambda r: np.logical_not(bool(re.search(\"\\/\",r))) & \\\n",
    "                  np.logical_not(bool(re.search(\"\\\\\\\\\",r))) & \\\n",
    "                  np.logical_not(bool(re.search(\"http\",r))) & \\\n",
    "                  np.logical_not(bool(re.search(\"www\",r)))  & \\\n",
    "                  np.logical_not(bool(re.search(\"comnode\",r))) & \\\n",
    "                  np.logical_not(bool(re.search(\"utm_\",r))) & \\\n",
    "                  np.logical_not(bool(re.search(\"v=\",r))) & \\\n",
    "                  np.logical_not(bool(re.search(\"lyw\",r)))\n",
    "                  , tweet_process_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14min 7s, sys: 4.39 s, total: 14min 11s\n",
      "Wall time: 14min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Use the NLTK package to tokenize each word in each sentence\n",
    "#Collect only adjectives and adverbs\n",
    "all_adj = []\n",
    "for sentence in good_sentences:\n",
    "    stokens = nltk.word_tokenize(sentence)\n",
    "    for word, part_of_speech in nltk.pos_tag(stokens):\n",
    "        if part_of_speech in ['JJ', 'JJS', 'JJR', 'RB', 'RBR', 'RBS', 'RP']:\n",
    "            all_adj.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4240"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove stop words\n",
    "full_corpus = filter(lambda r: r not in nltk.corpus.stopwords.words(\"english\"), all_adj)\n",
    "\n",
    "#Remove words that are contestant names\n",
    "full_corpus = filter(lambda r: r not in first_names, full_corpus)\n",
    "full_corpus = filter(lambda r: r not in bach_names, full_corpus)\n",
    "\n",
    "#Get unique values\n",
    "our_corpus = set(full_corpus)\n",
    "len(our_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Find sentiment of words in our corpus\n",
    "\n",
    "We use the website http://text-processing.com/, which is a library that gives the probability that a word is either positive or negative from a fit of words to movie and twitter data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#To not make the text-processing API mad, we will break up our data into 3 sections\n",
    "#text-processing.org throttles API requests to 1000 per day\n",
    "corpus1 = our_corpus[0:800]\n",
    "corpus2 = our_corpus[801:1600]\n",
    "corpus3 = our_corpus[1601:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def corpus_prob(which_word):\n",
    "\n",
    "    api_url = \"http://text-processing.com/api/sentiment/\"\n",
    "\n",
    "    data_type = {\"text\": which_word}\n",
    "    request_val = requests.post(api_url, data = data_type)\n",
    "\n",
    "    return json.loads(request_val.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.28 s, sys: 589 ms, total: 3.87 s\n",
      "Wall time: 16min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Run on CORPUS1\n",
    "json_list1 = []\n",
    "for wword in corpus1:\n",
    "    time.sleep(1)\n",
    "    json_list1.append(corpus_prob(wword))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Remove weird keys\n",
    "json_list1_good = []\n",
    "good_corpus1 = []\n",
    "\n",
    "for corp, dicts in zip(corpus1, json_list1):\n",
    "    try: \n",
    "        good_corpus1.append(corp.encode('utf-8'))\n",
    "        json_list1_good.append(dicts)\n",
    "    except:\n",
    "        \"\"\n",
    "probs1 = dict(zip(good_corpus1, json_list1_good))\n",
    "\n",
    "with open('probs1.json', 'w') as fp:\n",
    "    json.dump(probs1, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#Run on CORPUS2\n",
    "json_list2 = []\n",
    "for wword in corpus2:\n",
    "    time.sleep(1)\n",
    "    json_list2.append(corpus_prob(wword))\n",
    "    \n",
    "#Remove weird keys\n",
    "json_list2_good = []\n",
    "good_corpus2 = []\n",
    "\n",
    "for corp, dicts in zip(corpus2, json_list2):\n",
    "    try: \n",
    "        good_corpus2.append(corp.encode('utf-8'))\n",
    "        json_list2_good.append(dicts)\n",
    "    except:\n",
    "        \"\"\n",
    "probs2 = dict(zip(good_corpus2, json_list2_good))\n",
    "\n",
    "with open('probs2.json', 'w') as fp:\n",
    "    json.dump(probs2, fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.24 s, sys: 332 ms, total: 2.57 s\n",
      "Wall time: 12min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Run on CORPUS3\n",
    "json_list3 = []\n",
    "for wword in corpus3:\n",
    "    time.sleep(1)\n",
    "    json_list3.append(corpus_prob(wword))\n",
    "    \n",
    "#Remove weird keys\n",
    "json_list3_good = []\n",
    "good_corpus3 = []\n",
    "\n",
    "for corp, dicts in zip(corpus3, json_list3):\n",
    "    try: \n",
    "        good_corpus3.append(corp.encode('utf-8'))\n",
    "        json_list3_good.append(dicts)\n",
    "    except:\n",
    "        \"\"\n",
    "probs3 = dict(zip(good_corpus3, json_list3_good))\n",
    "\n",
    "with open('probs3.json', 'w') as fp:\n",
    "    json.dump(probs3, fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dictionaries for each contestant for each episode\n",
    "\n",
    "We want a dictionary that is keyed by season, contestant, and episode date. The values are then the share of tweets & a positivity index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read all dictionaries back in to get full corpus\n",
    "with open(\"probs1.json\") as json_file:\n",
    "    probs1 = json.load(json_file)\n",
    "with open(\"probs2.json\") as json_file:\n",
    "    probs2 = json.load(json_file)\n",
    "with open(\"probs3.json\") as json_file:\n",
    "    probs3 = json.load(json_file)\n",
    "\n",
    "corpus = {}\n",
    "corpus.update(probs1)\n",
    "corpus.update(probs2)\n",
    "corpus.update(probs3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Similar to full corpus tweet processing - make function to get single tweet into manageable format\n",
    "def tweet_process(tweet):\n",
    "    #Split sentences up\n",
    "    tweet_periods = tweet.split(\".\")\n",
    "    tweet_questions = [tp.split(\"?\") for tp in tweet_periods]\n",
    "    tweet_flat1 = [item for sublist in tweet_questions for item in sublist]\n",
    "    tweet_exclaim = [tq.split(\"!\") for tq in tweet_flat1]\n",
    "    tweet_flat2 = [item for sublist in tweet_exclaim for item in sublist]\n",
    "\n",
    "    #Replace hypens as spaces\n",
    "    tweet_sentences1 = [tf.replace(\"-\",\" \") for tf in tweet_flat2]\n",
    "    return [tf.replace(\"=\",\" \") for tf in tweet_sentences1]\n",
    "\n",
    "#Similar to full corpus, use a tweet to find all adjectives + adverbs for that tweet\n",
    "def tweet_part_of_speech(tweet_process_output):\n",
    "    \n",
    "    #Strip weird characters from words\n",
    "    tweet_encode = [tt.encode(\"ascii\", \"ignore\") for tt in tweet_process_output]\n",
    "    tweet_process_output = map(lambda r: r.translate(None,\"*@#\\/[]()\"), tweet_encode)\n",
    "    \n",
    "    for ch in uni_char:\n",
    "        tweet_process_output = map(lambda r: r.replace(ch,\"\"), tweet_encode)\n",
    "        \n",
    "    #Get all adjectives from tweet\n",
    "    all_adj = []\n",
    "    for sentence in tweet_process_output:\n",
    "        stokens = nltk.word_tokenize(sentence)\n",
    "        for word, part_of_speech in nltk.pos_tag(stokens):\n",
    "            if part_of_speech in ['JJ', 'JJS', 'JJR', 'RB', 'RBR', 'RBS', 'RP']:\n",
    "                all_adj.append(word)\n",
    "\n",
    "    good_adj = filter(lambda r: np.logical_not(bool(re.search(\"\\/\",r))) & \\\n",
    "                      np.logical_not(bool(re.search(\"\\\\\\\\\",r))) & \\\n",
    "                      np.logical_not(bool(re.search(\"http\",r))) & \\\n",
    "                      np.logical_not(bool(re.search(\"www\",r)))  & \\\n",
    "                      np.logical_not(bool(re.search(\"comnode\",r))) & \\\n",
    "                      np.logical_not(bool(re.search(\"utm_\",r))) & \\\n",
    "                      np.logical_not(bool(re.search(\"v=\",r))) & \\\n",
    "                      np.logical_not(bool(re.search(\"lyw\",r)))\n",
    "                      , all_adj)  \n",
    "    return good_adj\n",
    "\n",
    "#Make probability function that takes the output of tweet_part_of_speech and finds \n",
    "#the probability that the tweet is positive.\n",
    "#This essentially boils down to finding the mean positive probability of the tweet based on the \n",
    "#adjectives/adverbs. If above 50% - consider positive, otherwise negative.\n",
    "def is_tweet_positive(tweet_pos_output):\n",
    "    probs = []\n",
    "    for word in tweet_pos_output:\n",
    "        try:\n",
    "            word_prob = corpus[word]['label']\n",
    "            if word_prob == \"neg\":\n",
    "                probs.append(-1)\n",
    "            elif word_prob == \"pos\":\n",
    "                probs.append(1)\n",
    "            else:\n",
    "                probs.append(0)\n",
    "                \n",
    "        except:\n",
    "            \"\"\n",
    "    if len(probs) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.mean(probs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create function that takes season, gets tweets for each episode/contestant\n",
    "def tweets_by_season(use_season):\n",
    "    with open(\"tweets\"+str(use_season)+\".json\") as json_file:\n",
    "        tdat = json.load(json_file)\n",
    "\n",
    "    season_dates = date_guide[date_guide.Season == use_season]\n",
    "    season_dict={}\n",
    "    contestants = tdat.keys()\n",
    "\n",
    "    for contestant in contestants:\n",
    "        contestant_dict = {}\n",
    "        cont_dat = tdat[contestant]\n",
    "        if cont_dat is not None:\n",
    "            for cc in cont_dat:\n",
    "                episode_date = cc.keys()\n",
    "                number_of_tweets = 0\n",
    "                positive_index = 0\n",
    "                for tweet in cc[episode_date[0]]:\n",
    "                    number_of_tweets += 1\n",
    "                    positive_index += is_tweet_positive(tweet_part_of_speech(tweet_process(tweet)))\n",
    "                if number_of_tweets == 0:\n",
    "                    sentiment = 0\n",
    "                else:\n",
    "                    sentiment = float(positive_index) / float(number_of_tweets)\n",
    "\n",
    "                episode_dict = {episode_date[0]: {\"ntweet\": number_of_tweets, \"sentiment\":sentiment}}\n",
    "                contestant_dict.update(episode_dict)\n",
    "        season_dict.update({contestant: contestant_dict})\n",
    "    return season_dict               \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 is done\n",
      "14 is done\n",
      "15 is done\n",
      "16 is done\n",
      "17 is done\n",
      "18 is done\n",
      "19 is done\n",
      "CPU times: user 14min 49s, sys: 4.53 s, total: 14min 54s\n",
      "Wall time: 14min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Put all tweets together to form one large dictionary\n",
    "season_nums = range(13,20)\n",
    "tweet_dict = {}\n",
    "for season_num in season_nums:\n",
    "    dseason = tweets_by_season(season_num)\n",
    "    tweet_dict.update({season_num : dseason})\n",
    "    print season_num, \"is done\"\n",
    "    \n",
    "with open('twitter_sentiment.json', 'w') as fp:\n",
    "    json.dump(tweet_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
