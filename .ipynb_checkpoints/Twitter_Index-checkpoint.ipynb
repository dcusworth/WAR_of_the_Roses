{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code takes the libraries of data scraped from twitter and converts them into predictors that will be used for the future regression. We are interested in two types of predictors from the twitter data:\n",
    "\n",
    "1) The share of tweets a contestant received for one episode.\n",
    "\n",
    "2) The general positivity of a contestant's tweets for one episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import oauth2\n",
    "import simplejson\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "from pyquery import PyQuery as pq\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import datetime\n",
    "import json\n",
    "import unittest, time, re\n",
    "from pattern.en import parse\n",
    "from pattern.en import pprint\n",
    "from pattern.vector import stem, PORTER, LEMMA\n",
    "from sklearn.feature_extraction import text\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load Contestant Name Data from wiki scrape\n",
    "with open(\"tempdata/seasonsDict.json\") as json_file:\n",
    "    wiki_data = json.load(json_file)\n",
    "\n",
    "#Fix known formatting problems:\n",
    "wiki_data['19'][19]['eliminated'] = u'Eliminated in week 2'\n",
    "wiki_data['19'][20]['eliminated'] = u'Eliminated in week 1'\n",
    "\n",
    "w19 = []\n",
    "for ww in wiki_data['19'][0:29]:\n",
    "    w19.append(ww)\n",
    "    \n",
    "wiki_data['19'] = w19\n",
    "\n",
    "#Load date guide\n",
    "date_guide = pd.read_csv(\"date_guide.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get all contestant names\n",
    "cont_nam = []\n",
    "for wkey in wiki_data.keys():\n",
    "    for person in wiki_data[wkey]:\n",
    "        cont_nam.append(person['name'])\n",
    "\n",
    "#Strip names with weird formatting\n",
    "def url_strip(r):\n",
    "    if bool(re.search(\"href\", r)):\n",
    "        oval = r.split(\">\")[1].replace(\"</a\",\"\")\n",
    "    else:\n",
    "        oval = r\n",
    "    if bool(re.search(\"u\\\"\", oval)):\n",
    "        return oval.replace(\"u\\\"\",\"\").replace(\"[\",\"\")\n",
    "    else:\n",
    "        return oval\n",
    "    \n",
    "full_names = map(url_strip, cont_nam)\n",
    "\n",
    "#Get just first names\n",
    "first_names = set(map(lambda r: r.split(\" \")[0], full_names))\n",
    "\n",
    "bach_names = ['Jason', 'Jake', 'Brad', 'Ben', 'Sean', 'Juan', 'Chris']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Create Corpus\n",
    "\n",
    "Here we create a corpus of adjectives and adverbs from our entire body of tweets. Since Twitter is not copy-edited, we have to intensely filter our results to collect the words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First load all the tweets from all seasons\n",
    "all_tweets = []\n",
    "for iseason in range(12,20):\n",
    "    file_name = \"tweets\" + str(iseason) + \".json\"\n",
    "    with open(file_name) as json_file:\n",
    "        tdat = json.load(json_file)\n",
    "    \n",
    "    for tkey in tdat.keys():\n",
    "        cont_dat = tdat[tkey]\n",
    "        if cont_dat is not None:\n",
    "            for cc in cont_dat:\n",
    "                ep_dat = cc.keys()\n",
    "                for tweet in cc[ep_dat[0]]:\n",
    "                    all_tweets.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Next flatten all the tweets into one list - where one sentence = one list entry\n",
    "\n",
    "#Get all twitter sentences over all tweets   \n",
    "#Flatten all sentences into an array\n",
    "tweet_periods = map(lambda r: r.split(\".\"), all_tweets)\n",
    "tweet_flat1 = [item for sublist in tweet_periods for item in sublist]\n",
    "\n",
    "tweet_questions = map(lambda r: r.split(\"?\"), tweet_flat1)\n",
    "tweet_flat2 = [item for sublist in tweet_questions for item in sublist]\n",
    "\n",
    "tweet_exclaim = map(lambda r: r.split(\"!\"), tweet_flat2)\n",
    "tweet_flat3 = [item for sublist in tweet_exclaim for item in sublist]\n",
    "\n",
    "#Filter out empty sentences\n",
    "tweet_sentences = filter(lambda r: r not in \"\", tweet_flat3)\n",
    "\n",
    "#Replace hypens as spaces\n",
    "tweet_sentences = map(lambda r: r.replace(\"-\",\" \"), tweet_sentences)\n",
    "tweet_sentences = map(lambda r: r.replace(\"=\",\" \"), tweet_sentences)\n",
    "\n",
    "#Strip weird characters from words\n",
    "tweet_encode = [tt.encode(\"ascii\", \"ignore\") for tt in tweet_sentences]\n",
    "tweet_process_output = map(lambda r: r.translate(None,\"*@#\\/[]()\"), tweet_encode)\n",
    "\n",
    "#Filter out strange results in our vocabulary\n",
    "good_sentences = filter(lambda r: np.logical_not(bool(re.search(\"\\/\",r))) & \\\n",
    "                  np.logical_not(bool(re.search(\"\\\\\\\\\",r))) & \\\n",
    "                  np.logical_not(bool(re.search(\"http\",r))) & \\\n",
    "                  np.logical_not(bool(re.search(\"www\",r)))  & \\\n",
    "                  np.logical_not(bool(re.search(\"comnode\",r))) & \\\n",
    "                  np.logical_not(bool(re.search(\"utm_\",r))) & \\\n",
    "                  np.logical_not(bool(re.search(\"v=\",r))) & \\\n",
    "                  np.logical_not(bool(re.search(\"lyw\",r)))\n",
    "                  , tweet_process_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 12s, sys: 6.49 s, total: 15min 19s\n",
      "Wall time: 15min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Use the NLTK package to tokenize each word in each sentence\n",
    "#Collect only adjectives and adverbs\n",
    "all_adj = []\n",
    "for sentence in good_sentences:\n",
    "    stokens = nltk.word_tokenize(sentence)\n",
    "    for word, part_of_speech in nltk.pos_tag(stokens):\n",
    "        if part_of_speech in ['JJ', 'JJS', 'JJR', 'RB', 'RBR', 'RBS', 'RP']:\n",
    "            all_adj.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4146"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove stop words\n",
    "full_corpus = filter(lambda r: r not in nltk.corpus.stopwords.words(\"english\"), all_adj)\n",
    "\n",
    "#Remove words that are contestant names\n",
    "full_corpus = filter(lambda r: r not in first_names, full_corpus)\n",
    "full_corpus = filter(lambda r: r not in bach_names, full_corpus)\n",
    "\n",
    "#Get unique values\n",
    "our_corpus = set(full_corpus)\n",
    "len(our_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Find sentiment of words in our corpus\n",
    "\n",
    "We use the website http://text-processing.com/, which is a library that gives the probability that a word is either positive or negative from a fit of words to movie and twitter data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#To not make the text-processing API mad, we will break up our data into 5 sections\n",
    "#text-processing.org throttles API requests to 1000 per day\n",
    "corpus1 = list(our_corpus)[0:900]\n",
    "corpus2 = list(our_corpus)[901:1800]\n",
    "corpus3 = list(our_corpus)[1801:2700]\n",
    "corpus4 = list(our_corpus)[2701:3600]\n",
    "corpus5 = list(our_corpus)[3601:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create function that finds the positive probability of word\n",
    "def corpus_prob(which_word):\n",
    "\n",
    "    api_url = \"http://text-processing.com/api/sentiment/\"\n",
    "\n",
    "    data_type = {\"text\": which_word}\n",
    "    request_val = requests.post(api_url, data = data_type)\n",
    "\n",
    "    return json.loads(request_val.text)\n",
    "\n",
    "#Make function to find probabilities of a corpus\n",
    "#Returns dictionary of corpus words and probabilities\n",
    "def prob_of_corpus(which_corpus, corpus_num):\n",
    "\n",
    "    #Run on CORPUS1\n",
    "    json_list1 = []\n",
    "    for wword in which_corpus:\n",
    "        time.sleep(1)\n",
    "        json_list1.append(corpus_prob(wword))\n",
    "\n",
    "    #Remove weird keys\n",
    "    json_list1_good = []\n",
    "    good_corpus1 = []\n",
    "\n",
    "    for corp, dicts in zip(which_corpus, json_list1):\n",
    "        try: \n",
    "            good_corpus1.append(corp.encode('utf-8'))\n",
    "            json_list1_good.append(dicts)\n",
    "        except:\n",
    "            \"\"\n",
    "    probs1 = dict(zip(good_corpus1, json_list1_good))\n",
    "\n",
    "    with open('probs'+str(corpus_num)+'.json', 'w') as fp:\n",
    "        json.dump(probs1, fp)\n",
    "    \n",
    "    print \"corpus \", corpus_num, \" done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We retrieve probabilities from text-processing.com API. I do these in different cells so that I can change my IP address before running on each sub-corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus  1  done\n",
      "CPU times: user 3.4 s, sys: 560 ms, total: 3.96 s\n",
      "Wall time: 17min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prob_of_corpus(corpus1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus  2  done\n",
      "CPU times: user 3.23 s, sys: 446 ms, total: 3.68 s\n",
      "Wall time: 19min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prob_of_corpus(corpus2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus  3  done\n",
      "CPU times: user 3.18 s, sys: 498 ms, total: 3.68 s\n",
      "Wall time: 18min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prob_of_corpus(corpus3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus  4  done\n",
      "CPU times: user 3.32 s, sys: 482 ms, total: 3.8 s\n",
      "Wall time: 18min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prob_of_corpus(corpus4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus  5  done\n",
      "CPU times: user 1.97 s, sys: 301 ms, total: 2.27 s\n",
      "Wall time: 10min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prob_of_corpus(corpus5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dictionaries for each contestant for each episode\n",
    "\n",
    "We want a dictionary that is keyed by season, contestant, and episode date. The values are then the share of tweets & a positivity index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Read all dictionaries back in to get full corpus\n",
    "with open(\"probs1.json\") as json_file:\n",
    "    probs1 = json.load(json_file)\n",
    "with open(\"probs2.json\") as json_file:\n",
    "    probs2 = json.load(json_file)\n",
    "with open(\"probs3.json\") as json_file:\n",
    "    probs3 = json.load(json_file)\n",
    "with open(\"probs4.json\") as json_file:\n",
    "    probs4 = json.load(json_file)\n",
    "with open(\"probs5.json\") as json_file:\n",
    "    probs5 = json.load(json_file)\n",
    "\n",
    "corpus = {}\n",
    "corpus.update(probs1)\n",
    "corpus.update(probs2)\n",
    "corpus.update(probs3)\n",
    "corpus.update(probs4)\n",
    "corpus.update(probs5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Similar to full corpus tweet processing - make function to get single tweet into manageable format\n",
    "def tweet_process(tweet):\n",
    "    #Split sentences up\n",
    "    tweet_periods = tweet.split(\".\")\n",
    "    tweet_questions = [tp.split(\"?\") for tp in tweet_periods]\n",
    "    tweet_flat1 = [item for sublist in tweet_questions for item in sublist]\n",
    "    tweet_exclaim = [tq.split(\"!\") for tq in tweet_flat1]\n",
    "    tweet_flat2 = [item for sublist in tweet_exclaim for item in sublist]\n",
    "\n",
    "    #Replace hypens as spaces\n",
    "    tweet_sentences1 = [tf.replace(\"-\",\" \") for tf in tweet_flat2]\n",
    "    return [tf.replace(\"=\",\" \") for tf in tweet_sentences1]\n",
    "\n",
    "#Similar to full corpus, use a tweet to find all adjectives + adverbs for that tweet\n",
    "def tweet_part_of_speech(tweet_process_output):\n",
    "    \n",
    "    #Strip weird characters from words\n",
    "    tweet_encode = [tt.encode(\"ascii\", \"ignore\") for tt in tweet_process_output]\n",
    "    tweet_process_output = map(lambda r: r.translate(None,\"*@#\\/[]()\"), tweet_encode)\n",
    "        \n",
    "    #Get all adjectives from tweet\n",
    "    all_adj = []\n",
    "    for sentence in tweet_process_output:\n",
    "        stokens = nltk.word_tokenize(sentence)\n",
    "        for word, part_of_speech in nltk.pos_tag(stokens):\n",
    "            if part_of_speech in ['JJ', 'JJS', 'JJR', 'RB', 'RBR', 'RBS', 'RP']:\n",
    "                all_adj.append(word)\n",
    "\n",
    "    good_adj = filter(lambda r: np.logical_not(bool(re.search(\"\\/\",r))) & \\\n",
    "                      np.logical_not(bool(re.search(\"\\\\\\\\\",r))) & \\\n",
    "                      np.logical_not(bool(re.search(\"http\",r))) & \\\n",
    "                      np.logical_not(bool(re.search(\"www\",r)))  & \\\n",
    "                      np.logical_not(bool(re.search(\"comnode\",r))) & \\\n",
    "                      np.logical_not(bool(re.search(\"utm_\",r))) & \\\n",
    "                      np.logical_not(bool(re.search(\"v=\",r))) & \\\n",
    "                      np.logical_not(bool(re.search(\"lyw\",r)))\n",
    "                      , all_adj)  \n",
    "    return good_adj\n",
    "\n",
    "#Make probability function that takes the output of tweet_part_of_speech and finds \n",
    "#the probability that the tweet is positive.\n",
    "#This essentially boils down to finding the mean positive probability of the tweet based on the \n",
    "#adjectives/adverbs. If above 50% - consider positive, otherwise negative.\n",
    "def is_tweet_positive(tweet_pos_output):\n",
    "    probs = []\n",
    "    for word in tweet_pos_output:\n",
    "        try:\n",
    "            word_prob = corpus[word]['label']\n",
    "            if word_prob == \"neg\":\n",
    "                probs.append(-1)\n",
    "            elif word_prob == \"pos\":\n",
    "                probs.append(1)\n",
    "            else:\n",
    "                probs.append(0)\n",
    "                \n",
    "        except:\n",
    "            \"\"\n",
    "    if len(probs) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.mean(probs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create function that takes season, gets tweets for each episode/contestant\n",
    "def tweets_by_season(use_season):\n",
    "    with open(\"tweets\"+str(use_season)+\".json\") as json_file:\n",
    "        tdat = json.load(json_file)\n",
    "\n",
    "    season_dates = date_guide[date_guide.Season == use_season]\n",
    "    season_dict={}\n",
    "    contestants = tdat.keys()\n",
    "\n",
    "    for contestant in contestants:\n",
    "        contestant_dict = {}\n",
    "        cont_dat = tdat[contestant]\n",
    "        if cont_dat is not None:\n",
    "            for cc in cont_dat:\n",
    "                episode_date = cc.keys()\n",
    "                number_of_tweets = 0\n",
    "                positive_index = 0\n",
    "                for tweet in cc[episode_date[0]]:\n",
    "                    number_of_tweets += 1\n",
    "                    positive_index += is_tweet_positive(tweet_part_of_speech(tweet_process(tweet)))\n",
    "                if number_of_tweets == 0:\n",
    "                    sentiment = 0\n",
    "                else:\n",
    "                    sentiment = float(positive_index) / float(number_of_tweets)\n",
    "\n",
    "                episode_dict = {episode_date[0]: {\"ntweet\": number_of_tweets, \"sentiment\":sentiment}}\n",
    "                contestant_dict.update(episode_dict)\n",
    "        season_dict.update({contestant: contestant_dict})\n",
    "    return season_dict               \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 is done\n",
      "14 is done\n",
      "15 is done\n",
      "16 is done\n",
      "17 is done\n",
      "18 is done\n",
      "19 is done\n",
      "CPU times: user 13min 47s, sys: 5.63 s, total: 13min 53s\n",
      "Wall time: 13min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Put all tweets together to form one large dictionary\n",
    "season_nums = range(13,20)\n",
    "tweet_dict = {}\n",
    "for season_num in season_nums:\n",
    "    dseason = tweets_by_season(season_num)\n",
    "    tweet_dict.update({season_num : dseason})\n",
    "    print season_num, \"is done\"\n",
    "    \n",
    "with open('twitter_sentiment.json', 'w') as fp:\n",
    "    json.dump(tweet_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
