{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Scrape\n",
    "\n",
    "Here we scrape wikipedia pages that contain information about each season of The Bachelor. We are interested in the following fundamental data types: Age, Profession, Hometown. Additionally, for each week of the compeition, we scrape to find out if a contestant received a group date, a group rose, or a one-on-one date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "from pyquery import PyQuery as pq\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from competition_data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get the raw HTML from each page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get wiki for all bachelor seasons\n",
    "allseasons = requests.get(\"https://en.wikipedia.org/wiki/The_Bachelor_(U.S._TV_series)#Seasons\")\n",
    "soup = BeautifulSoup(allseasons.text, \"html.parser\") #make soup element\n",
    "\n",
    "#get the table cell that has links to each episode\n",
    "seasons = soup.find(\"table\", attrs={\"class\":\"navbox\"}).find(\"td\", attrs={\"class\":\"navbox-list navbox-odd hlist\"})\n",
    "seasons = seasons.find(\"div\", attrs={\"style\":\"padding:0em 0.25em\"}).find(\"ul\")\n",
    "\n",
    "urls = []                           #list of links to season-specific page\n",
    "seasonNums = []                     #list of seasons w/ wiki pages (no seasons 1-4 or 6-8)\n",
    "seasonNum = 1                       #season number\n",
    "for item in seasons.find_all(\"li\"): #for each item in list of seasons\n",
    "    if (seasonNum == 20):           #don't include season 20, b/c no contestants listed yet\n",
    "        break\n",
    "    season = item.find(\"a\")         #get url tag\n",
    "    if season is not None:          #if has url link, get url text\n",
    "        urls.append(\"\\\"https://en.wikipedia.org\" + season.get(\"href\") + \"\\\"\")\n",
    "        seasonNums.append(seasonNum) #add season number to list \n",
    "    seasonNum += 1\n",
    "    \n",
    "wikiPageText = []                   #init list of wiki site text, for all seasons\n",
    "for url in urls:\n",
    "    site = requests.get(url[1:-1])  #get web-site for that url\n",
    "    soup = BeautifulSoup(site.text, \"html.parser\") #make BS element\n",
    "    wikiPageText.append(soup)       #add web-site text to list\n",
    "\n",
    "wikiPages = dict(zip(seasonNums, wikiPageText)) #key=season, val=Soup Elem(wiki page text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we parse through the tags of the HTML to get the fundamental data sources we are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For each season in wiki, make list of dictionaries - one dictionary for each contestant.\n",
    "# Also, make a list of dictionaries of all contestants for all seasons.\n",
    "#\n",
    "# list name       = listAllDicts  #a list of all dicts for all contestants and bachelors, all seasons\n",
    "#\n",
    "# dictionary name = seasonsDict\n",
    "#             key = season number\n",
    "#           value = list of dictionaries for that season (one for each contestant)\n",
    "#             \n",
    "# dictionary name = contestantDict\n",
    "#            keys = name, age, hometown, occupation, elimination, season\n",
    "#          values = associated values to fields, as scraped from wiki\n",
    "#\n",
    "# To test contestant dictionaries:\n",
    "#         print seasonsDict[season][contestant][fieldname]\n",
    "#    eg:  print seasonsDict[9][10]['name']  -- get name for season 9, contestant 10\n",
    "#\n",
    "# Note: Wiki does not have pages dedicated to Seasons 1-4, or 6-8. I added 2, 4, 6, and 8\n",
    "# below, from other sources. Contestants for episode 20 are not added, because they are\n",
    "# not public yet.\n",
    "#\n",
    "import sys  \n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "seasonsDict = dict()                #key = season num, val=list of contestant dictionaries\n",
    "allContestants = dict()             #keys = name/age/etc, values = associated data\n",
    "listAllDicts = []                   #list of dicts for all cont. and bach. for ALL seasons\n",
    "\n",
    "for sn in seasonNums:\n",
    "    seasonPage = wikiPages[sn]      #get BS element for this season\n",
    "    seasonPage = seasonPage.find(\"div\", attrs={\"id\":\"content\"}).find(\"div\", attrs={\"id\":\"bodyContent\"})\n",
    "    seasonPage = seasonPage.find(\"div\", attrs={\"id\":\"mw-content-text\"})\n",
    "    seasonPage = seasonPage.find(\"table\", attrs={\"class\":\"wikitable sortable\"})\n",
    "    \n",
    "    listOfContestantDicts = []          #list of dicts for each contestant\n",
    "    \n",
    "    numtr = 0                           #num rows (one per contestant)\n",
    "    for tr in seasonPage.find_all(\"tr\"):#for each contestant listed,\n",
    "        if (numtr == 0):                #skip first row (column headers)\n",
    "            numtr += 1\n",
    "            continue\n",
    "\n",
    "        contestantDict = dict()         #init new dict for contestant\n",
    "        numtd = 0                       #column number\n",
    "        for td in tr.find_all(\"td\"):    #for each column of data,\n",
    "            \n",
    "            #NAME\n",
    "            if (numtd == 0):\n",
    "                name = str(td.contents)\n",
    "                if (\"<b>\" in name):\n",
    "                    td.find(\"b\")\n",
    "                    name = str(td.contents)[4:-5]\n",
    "                if ((\"[u'\" in name) or (\"[u\\\"\" in name)):   #if \"[u'name']\",\n",
    "                    name = name.encode('utf8')[3:-2]    #format to get 'name'\n",
    "                if (\"<span class\" in name):\n",
    "                    td.find(\"span\", attrs={\"class\":\"nowrap\"})\n",
    "                    tag = \"<span class='nowrap'>\"       #start tag before name\n",
    "                    name = str(td.contents)[len(tag)+1:]#cut out start tag\n",
    "                    end = name.index(\"<\")               #get start point of end tag\n",
    "                    name = name[:end]                   #cut out end tag\n",
    "                    trashTag = \"style=\\\"display:none;\\\">\" #weird tag to cut from a name\n",
    "                    if (trashTag in name):\n",
    "                        name = name[(len(trashTag)+1):-1] \n",
    "                if (\"<sup\" in name):                    #if name has \"name', <sup ...\",\n",
    "                    end2 = name.index(\"<sup\")           #format to get name\n",
    "                    name = name[:end2-3]\n",
    "                if (\"</b\" in name):\n",
    "                    name = name[:name.index(\"</b\")]\n",
    "                if (\"href\" in name):                    #if name has url\n",
    "                    name = td.find(\"a\").get(\"href\")\n",
    "                    name = td.get_text(\"title\")\n",
    "                if (\"title\" in name):                   #if 'title' in name, take it out\n",
    "                    name = name[:name.index(\"title\")]   #eg \"Keltie Colleentitle[title20title]\"\n",
    "                contestantDict['name'] = name           #add name to dict\n",
    "                \n",
    "            #AGE\n",
    "            if (numtd == 1):\n",
    "                age = str(td.contents)\n",
    "                if (\"<b>\" in age):\n",
    "                    td.find(\"b\")\n",
    "                    age = str(td.contents)[4:-5]\n",
    "                if (\"[u'\" in age):                      \n",
    "                    age = age.encode('utf8')[3:5]\n",
    "                if (age is None):                   #if no age (eg season 9, Cosetta Blanca)\n",
    "                    age = \"na\"\n",
    "                contestantDict['age'] = age\n",
    "                \n",
    "            #HOME\n",
    "            if (numtd == 2):\n",
    "                home = \"\"\n",
    "                for url in td.find_all(\"a\"):        #for each url to a place,\n",
    "                    url.get(\"href\")\n",
    "                    home2 = url.get_text(\"title\")   #get place name\n",
    "                    if (len(home) > 0):             #if already have city,\n",
    "                        home = home + \", \" + home2  #concatenate state\n",
    "                    else:                           #if no city yet (or home is one word),\n",
    "                        home = home2                #save city name or home name\n",
    "\n",
    "                if (\"title\" in home):               #format oddity in season 19, contest 1\n",
    "                    indx = home.index(\"title\")\n",
    "                    home = home[:indx]\n",
    "                if (\"[\" in home):                   #format oddity - homes end in \", [\"\n",
    "                    indx2 = home.index(\"[\")         \n",
    "                    home = home[:indx2-2]\n",
    "                \n",
    "                contestantDict['hometown'] = home\n",
    "\n",
    "                \n",
    "            #OCCUPATION\n",
    "            if (numtd == 3):\n",
    "                job = str(td.contents)\n",
    "                if (\"<b>\" in job):\n",
    "                    td.find(\"b\")\n",
    "                    job = str(td.contents)[4:-5]\n",
    "                if ((\"[u'\" in job) or (\"[u\\\"\" in job)):               \n",
    "                    job = job.encode('utf8')[3:-2]  \n",
    "                if (\"href\" in job):                 #if occupation has url\n",
    "                    job = td.find(\"a\").get(\"href\")\n",
    "                    job = td.get_text(\"title\")\n",
    "                if (\"nowrap\" in job):\n",
    "                    job = td.get_text(\"span\") #, attr={\"class\":\"nowrap\"})\n",
    "                if (\"title\" in job):\n",
    "                    titleindex = job.index(\"title\")\n",
    "                    job = job[:titleindex] + \" \" + job[(titleindex+len(\"title\")):]\n",
    "                if (\"title\" in job):                #sometimes, 'title' appears twice in 'occupation'\n",
    "                    titleindex = job.index(\"title\")\n",
    "                    job = job[:titleindex] + \" \" + job[(titleindex+len(\"title\")):]\n",
    "                if (\"below\" in job):\n",
    "                    job = \"unknown\"\n",
    "                contestantDict['occupation'] = job   \n",
    "                \n",
    "            #ELIMINATION\n",
    "            if (numtd == 4):\n",
    "                elim = str(td.contents)\n",
    "                if (\"<b>\" in elim):\n",
    "                    td.find(\"b\")\n",
    "                    elim = str(td.contents)[4:-5]\n",
    "                if (\"[u'\" in elim):                      \n",
    "                    elim = elim.encode('utf8')[3:-2] \n",
    "                if(\"Eliminated in \" in elim):\n",
    "                    elim = elim[len(\"Eliminated in \"):]\n",
    "                if((\"Quit in \" in elim) or (\"quit in \" in elim)):\n",
    "                    elim = elim[len(\"Quit in \"):]\n",
    "                if((\"Week \" in elim) or (\"week \" in elim)):  #remove \"week\", leave week number only\n",
    "                    elim = elim[len(\"Week \"):]\n",
    "                if ((\"Returned\" in elim) or (\"returned\" in elim)):\n",
    "                    elim = elim[:elim.index(\"', <br/>\")]\n",
    "                contestantDict['elimination'] = elim\n",
    "\n",
    "            numtd += 1\n",
    "        numtr += 1\n",
    "        \n",
    "        contestantDict['season'] = sn   #include season num in dict\n",
    "        \n",
    "        listOfContestantDicts.append(contestantDict) #add dict to list of dicts in this season\n",
    "        listAllDicts.append(contestantDict)         #add dict to list of all dicts in all seasons\n",
    "    seasonsDict[sn] = listOfContestantDicts  #key = season num, val=list of contestant dicts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get the \"competitive\" data - i.e. whether a contestant received a rose in a given week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from types import *\n",
    "from collections import Counter\n",
    "import operator\n",
    "import re\n",
    "\n",
    "def getCompetitionDetails(wikiPages, seasons):\n",
    "    competitionDetails = dict()\n",
    "    newerSeasons = [18,19] # these season have a different page layout and must be handled differently\n",
    "    for sn in seasons:\n",
    "        weeklyDetails = []\n",
    "        seasonPage = wikiPages[sn]      #get BS element for this season\n",
    "        seasonPage = seasonPage.find(\"div\", attrs={\"id\":\"content\"}).find(\"div\", attrs={\"id\":\"bodyContent\"})\n",
    "        seasonPage = seasonPage.find(\"div\", attrs={\"id\":\"mw-content-text\"})\n",
    "        bodyElements = seasonPage.find_all(['h2','h3','p','table']) #put body elements into a list so they can be parsed through\n",
    "        \n",
    "\n",
    "        # iterater over the list of body elements to find the relevent data\n",
    "        for i, element in enumerate(bodyElements):\n",
    "            if sn in newerSeasons:\n",
    "                # find the episodes section\n",
    "                if element.name == 'h2' and not type(element.span) == NoneType:\n",
    "                    if element.span.text == \"Episodes\":\n",
    "                        # capture each table row into the episodesDetail list excluding \n",
    "                        # the first tr, which is the table header.\n",
    "                        # the rows are arranged in pairs, with the first the row containing \n",
    "                        # the episode num, title and air date and the second row containing\n",
    "                        # the episode summary\n",
    "                        episodesDetails = bodyElements[i+1].find_all('tr')\n",
    "                        for episodeIndex, episodeDetails in enumerate(episodesDetails):\n",
    "                            \n",
    "                            \n",
    "                            if not episodeIndex % 2 == 0:\n",
    "                                episodeNumber = (episodeIndex+1) / 2\n",
    "                                if episodeIndex+1 < len(episodesDetails):\n",
    "                                    episodeSummary = episodesDetails[episodeIndex+1].text\n",
    "                                    episodeTitle = episodeDetails.find('td', {'class':'summary'}).text\n",
    "                                    if \"Week\" in episodeTitle:\n",
    "                                        weeklyDetails.append(episodeSummary)\n",
    "                                    \n",
    "                # get the tab\n",
    "            else:\n",
    "                text = element.find(text=True)\n",
    "                if \"Week\" in str(text):\n",
    "                    parsingSection = True\n",
    "                    nextElementIndex = i+1\n",
    "                    # parse through the next elements until we reach \n",
    "                    # elements pertaining to the next week\n",
    "                    while parsingSection:\n",
    "                        if not (nextElementIndex < len(bodyElements)):\n",
    "                            break\n",
    "\n",
    "                        nextElement = bodyElements[nextElementIndex]\n",
    "                        # if the next element is an h3 element then it is the header of the \n",
    "                        # next we and we should stop here\n",
    "                        if nextElement.name == 'h2' or nextElement.name == 'h3':\n",
    "                            parsingSection = False\n",
    "                            weeklyDetails.append(bodyElements[i+1:nextElementIndex])\n",
    "                        # otherwise we go to the next element\n",
    "                        else:\n",
    "                            nextElementIndex += 1\n",
    "\n",
    "\n",
    "        competitionDetails[sn] = weeklyDetails \n",
    "\n",
    "    return competitionDetails\n",
    "\n",
    "\n",
    "\n",
    "def getContestantCompData(competitionDetails, allContestants):\n",
    "    contestantCompData = {} \n",
    "    newerSeasons = [18, 19]\n",
    "    \n",
    "    for sn in list(allContestants.keys()):\n",
    "        contestantsData = {}\n",
    "        # for each episode determine who had a group or one on one date and whether they got a rose \n",
    "        for weekIndex, weekDetails in enumerate(competitionDetails[sn]):\n",
    "            if sn in newerSeasons:\n",
    "                episodeSummary = weekDetails\n",
    "                hasIndividualDates = 'one-on-one' in episodeSummary.lower() or 'two-on-one' in episodeSummary.lower()\n",
    "                hasGroupDates = 'group date:' in episodeSummary.lower()\n",
    "                \n",
    "                if hasIndividualDates or hasGroupDates:\n",
    "                    for marker in ['one-on-one', 'group date:', 'two-on-one']:\n",
    "                        dateType = marker.replace(' date:','') \n",
    "                        if not marker in episodeSummary.lower():\n",
    "                            continue\n",
    "                        # find all indexes of the markers specified\n",
    "                        markerIndexes = [m.end() for m in re.finditer(marker, episodeSummary.lower())]\n",
    "                        for markerIndex in markerIndexes:                        \n",
    "                            endOfContestants = markerIndex + episodeSummary[markerIndex:].index('. ')\n",
    "                            contestants = episodeSummary[markerIndex:endOfContestants]\n",
    "                            endOfSection = episodeSummary[markerIndex:].index('\\n') + markerIndex\n",
    "                            section = episodeSummary[markerIndex:endOfSection]\n",
    "                            generateRoseData(allContestants[sn], contestantsData, weekIndex+1, dateType, contestants, section)\n",
    "\n",
    "            else:\n",
    "                for detailsIndex, details in enumerate(weekDetails):\n",
    "                    if not(type(details.b) == NoneType):\n",
    "                        dateType = details.b.text.lower()\n",
    "                        if \"one-on-one\" in dateType or \"two-on-one\" in dateType or \"group\" in dateType:\n",
    "                            start = details.text.index(':') +2\n",
    "                            end = details.text.index('. ')\n",
    "                            contestants = details.text[start:end]\n",
    "                            generateRoseData(allContestants[sn], contestantsData, weekIndex+1, dateType, contestants, details.text)\n",
    "                                    \n",
    "        contestantCompData[sn] = contestantsData\n",
    "        \n",
    "    return contestantCompData\n",
    "  \n",
    "def generateRoseData(allContestants, contestantsData, week, dateType, contestantsOnDate, section):\n",
    "\n",
    "    # find the contestants that are on the date \n",
    "    contestants = []\n",
    "    for contestant in allContestants:\n",
    "        contestantFirstName = contestant['name'].split(' ')[0]\n",
    "        if contestantFirstName in contestantsOnDate:\n",
    "            contestants.append(contestantFirstName)\n",
    "    # account for any contestants with same name\n",
    "    contestants = dict(Counter(contestants))\n",
    "    contestantsToRemove = []\n",
    "    contestantsToAdd = []\n",
    "    for name in contestants:\n",
    "        if contestants[name] > 1:\n",
    "            contestantsToRemove.append(name)\n",
    "            for contestant in allContestants:\n",
    "                if name in contestant['name']:\n",
    "                    fullName = contestant['name'].split(' ')\n",
    "                    if len(fullName) == 3:\n",
    "                        firstName, middleName, lastName = fullName\n",
    "                        if '(' in lastName and ')' in lastName:\n",
    "                            ref = lastName\n",
    "                            lastName = middleName\n",
    "                            searchableName = firstName+' '+ref\n",
    "                    else:\n",
    "                        firstName, lastName = fullName\n",
    "                        searchableName = firstName+' '+lastName[0]\n",
    "                        #searchableName2 = firstName+lastName[0]\n",
    "                    if searchableName in contestantsOnDate:\n",
    "                        contestantsToAdd.append(searchableName)\n",
    "\n",
    "    for name in contestantsToRemove:\n",
    "        del contestants[name]\n",
    "\n",
    "    for name in contestantsToAdd:\n",
    "        contestants[name] = 1\n",
    "\n",
    "    contestants = contestants.keys()\n",
    "    \n",
    "\n",
    "    # determine who got a rose\n",
    "    if 'rose' in section:\n",
    "        roseIndex = section.index('rose')\n",
    "        endOfSentence = section[roseIndex:].index('.') + roseIndex\n",
    "        startOfSentence = endOfSentence - section[:endOfSentence][::-1].index('.') + 1\n",
    "        roseSentence = section[startOfSentence:endOfSentence]\n",
    "        \n",
    "        for contestant in contestants:\n",
    "            contestantFirstName = contestant.split(' ')[0]\n",
    "            receivedRose = False\n",
    "            for got in ['got', 'receiv', 'gets', 'presents', 'gives','holds','has','giving', 'gave', 'extend']:\n",
    "                if got in roseSentence and not (' not ' in roseSentence):\n",
    "                    if contestantFirstName in roseSentence:\n",
    "                        receivedRose = True\n",
    "                        break\n",
    "                    elif 'one' in dateType: # we assume the pronoun refers to contestant on the date\n",
    "                            receivedRose = True\n",
    "                            break\n",
    "                    elif 'group' in dateType: # we assume the pronoun refers to the last mentioned contestant\n",
    "                        contestantIndex = {}\n",
    "                        for c in contestants:\n",
    "                            contestantIndex[c.split(' ')[0]] = section[:endOfSentence][::-1].index(c.split(' ')[0][::-1])\n",
    "\n",
    "                        closestContestant = min(contestantIndex.iteritems(), key=operator.itemgetter(1))[0]\n",
    "                        if closestContestant in contestant:\n",
    "                            receivedRose = True\n",
    "                            break\n",
    "                            \n",
    "            compData = (week, dateType, receivedRose)\n",
    "\n",
    "            if contestant in contestantsData:\n",
    "                contestantsData[contestant].append(compData)\n",
    "            else:\n",
    "                contestantsData[contestant] = [compData] \n",
    "def getWeeklyCompData(contestantCompData):\n",
    "    result = {}\n",
    "    for sn in contestantCompData:\n",
    "        weeklyCompData = {}\n",
    "        for contestant in contestantCompData[sn]: \n",
    "            dates = contestantCompData[sn][contestant]\n",
    "            for week,dateType,receivedRose in dates:\n",
    "                if not week in weeklyCompData:\n",
    "                    weeklyCompData[week] = {}\n",
    "                if not contestant in weeklyCompData[week]:\n",
    "                    weeklyCompData[week][contestant] = {}\n",
    "                if not dateType in weeklyCompData[week][contestant]:\n",
    "                    weeklyCompData[week][contestant][dateType] = {}\n",
    "\n",
    "                weeklyCompData[week][contestant][dateType] = receivedRose\n",
    "        result[sn] = weeklyCompData\n",
    "    return result\n",
    "\n",
    "def addCompetitionData(seasonsDict, contestantCompData):\n",
    "    for sn in seasonsDict:    \n",
    "        for contestant in seasonsDict[sn]:\n",
    "            contestant['group_dates'] = 0\n",
    "            contestant['individual_dates'] = 0\n",
    "            contestant['roses_from_group_dates'] =  0\n",
    "            contestant['roses_from_individual_dates'] = 0\n",
    "            for contestantFirstName in contestantCompData[sn]:\n",
    "                contestantName = \"None\"\n",
    "                if contestantFirstName == \"\" or \" \" in contestantFirstName or len(contestantFirstName) < 3:\n",
    "                    continue\n",
    "                if contestantFirstName[-1].isupper():\n",
    "                    contestantName = contestantFirstName[:-1] + ' ' + contestantFirstName[-1]\n",
    "\n",
    "                if contestantFirstName in contestant['name'] or contestantName in contestant['name']:\n",
    "                    dates = contestantCompData[sn][contestantFirstName]\n",
    "                    numGroupDates = 0\n",
    "                    numIndividualDates = 0\n",
    "                    numRoseOnGroupDates = 0\n",
    "                    numRoseOnIndividualDates = 0\n",
    "                    for week, dateType, receivedRose in dates:\n",
    "                        if 'group' in dateType.lower():\n",
    "                            numGroupDates += 1\n",
    "                            if receivedRose:\n",
    "                                numRoseOnGroupDates += 1\n",
    "                        if 'one' in dateType.lower():\n",
    "                            numIndividualDates += 1\n",
    "                            if receivedRose:\n",
    "                                numRoseOnIndividualDates += 1\n",
    "                        \n",
    "                    \n",
    "                    contestant['group_dates'] = numGroupDates\n",
    "                    contestant['individual_dates'] = numIndividualDates\n",
    "                    contestant['roses_from_group_dates'] =  numRoseOnGroupDates\n",
    "                    contestant['roses_from_individual_dates'] = numRoseOnIndividualDates\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the details of each competition for each week\n",
    "# compettionDetails is a dictionary: key = season number, val = list of weekly summary \n",
    "competitionDetails = getCompetitionDetails(wikiPages, seasonsDict) \n",
    "# Get the detailts of each contestants performances for each season\n",
    "# contestantCompData is a dictionary: key = season number, val = dict with contestant \n",
    "# name as keys (values are tuples = (week num [int], date type [str], received rose [bool]))\n",
    "contestantCompData = getContestantCompData(competitionDetails, seasonsDict)\n",
    "# Get each contestants performance for each week of each season\n",
    "# weeklyCompData is a dict: key = season number, val = dict with week number as keys \n",
    "# (values are dict: key = contestant name, val = dict: key=date type val=reeived rose (bool))\n",
    "weeklyCompData = getWeeklyCompData(contestantCompData)\n",
    "# Add the contestantCompData to contestant dictionaries in seasonsDict\n",
    "addCompetitionData(seasonsDict, contestantCompData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now add the fundamental data for each Bachelor - i.e. where is he from, what is his job, how old is he."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add the men (the Bachelors) for all seasons. \n",
    "#\n",
    "# FIRST: get all data and add it to a dictionary, one for each bachelor:\n",
    "#     dictionary name = bachelorDict\n",
    "#                keys = name, age, hometown, occupation, elimination=bachelor, season\n",
    "#              values = associated values to fields, as scraped from wiki\n",
    "#\n",
    "#     For example, here is the dictionary for the first bachelor (Season 1):\n",
    "#          {'name': 'Alex Michel', 'hometown': 'Charlottesville, Virginia', \n",
    "#          'age': 32, 'season': '1', 'elimination': 'bachelor', \n",
    "#          'occupation': 'Management consultant'}\n",
    "#\n",
    "# SECOND: add this data to the list that has data for all contestans and bachelors:\n",
    "#      listAllDicts    -   a list of all dicts for all cont and bachelors, all seasons\n",
    "#\n",
    "# Bachelors can be identified easily in this list because their 'elimination' column value \n",
    "# is 'bachelor' (whereas contestants have 'elimination' column values 'runner-up', or \n",
    "# '7' for week seven).\n",
    "#\n",
    "\n",
    "#go to wiki homepage for The Bachelor, make soup element\n",
    "allseasons = requests.get(\"https://en.wikipedia.org/wiki/The_Bachelor_(U.S._TV_series)#Seasons\")\n",
    "soup = BeautifulSoup(allseasons.text, \"html.parser\") \n",
    "\n",
    "#get the table cell that has links to each episode\n",
    "men = soup.find(\"table\", attrs={\"class\":\"wikitable plainrowheaders\"})\n",
    "men = men.find_all(\"tr\")\n",
    "\n",
    "numRow = 0\n",
    "for man in men:                                 #for each bachelor in the table,\n",
    "    bachelorDict = dict()                       #init new dict for this bachelor\n",
    "    bachAge = \"unknown\"                         #default values for those we can't find\n",
    "    bachHometown = \"unknown\"\n",
    "    bachOccupation = \"unknown\"\n",
    "    if (numRow == 0):                           #skip first row - col titles\n",
    "        numRow += 1\n",
    "        continue\n",
    "    if (numRow > 19):                           #don't collect data on Season 20 Bachelor\n",
    "        break\n",
    "        \n",
    "    numCol = 0\n",
    "    for col in man.find_all(\"td\"):              #for each col in this bachelor row,       \n",
    "\n",
    "        #SEASON NUMBER\n",
    "        if (numCol == 0):                              \n",
    "            seasonNum = col.get_text()\n",
    "            seasonNum = seasonNum.encode('utf8')\n",
    "        \n",
    "        if (numCol == 1):                       #get season year (to calculate age later)\n",
    "            col = col.get_text()\n",
    "            if (\"[\" in col):\n",
    "                seasonYear =  col.encode('utf8')[-8:-4]\n",
    "            else:\n",
    "                seasonYear =  col.encode('utf8')[-4:]\n",
    "        \n",
    "        #Get URL for bachelor's personal site, make soup element \n",
    "        if (numCol == 2):                               \n",
    "            manURL = col.find(\"a\").get(\"href\")\n",
    "            manPage = requests.get(\"https://en.wikipedia.org\" + manURL)\n",
    "            bachSoup = BeautifulSoup(manPage.text, \"html.parser\") \n",
    "            manSoup = bachSoup.find(\"table\", attrs={\"class\":\"infobox biography vcard\"})\n",
    "            if (manSoup is None):\n",
    "                manSoup = bachSoup.find(\"table\", attrs={\"class\":\"infobox vcard\"})\n",
    "            if (manSoup is not None):\n",
    "                manSoup = manSoup.find_all(\"tr\")\n",
    "            \n",
    "                #go to 'biography box' on bachelor's personal site\n",
    "                bioRow = 0\n",
    "                for row in manSoup:\n",
    "                    #BACHELOR AGE\n",
    "                    bornYear = row.find(\"span\", attrs={\"class\":\"bday\"})\n",
    "                    if (bornYear is not None):\n",
    "                        bornYear = bornYear.get_text()\n",
    "                        bornYear = bornYear.encode('utf8')[:4]\n",
    "                        bachAge = int(seasonYear) - int(bornYear)   #calculate age\n",
    "                    \n",
    "                    #BACHELOR HOMETOWN\n",
    "                    bachHome = row.find(\"span\", attrs={\"class\":\"birthplace\"})\n",
    "                    if (bachHome is None):\n",
    "                        bachHome = row.find(\"td\", attrs={\"class\":\"birthplace\"})\n",
    "                    if (bachHome is not None):\n",
    "                        bachHometown = bachHome.find(\"a\") #.get(\"href\")\n",
    "                        bachHometown = str(bachHometown.contents[0])\n",
    "                    if (\"New York City\" in bachHometown):\n",
    "                        bachHometown = \"New York City, New York\"\n",
    "\n",
    "                    bioRow += 1\n",
    "\n",
    "            #BACHELOR NAME\n",
    "            bachName = col.get_text()\n",
    "            bachName = bachName.encode('utf8')\n",
    "            if (\"[\" in bachName):\n",
    "                bachName = bachName[:-4]\n",
    "                \n",
    "\n",
    "        #BACHELOR OCCUPATION\n",
    "        if (numCol == 3):                               \n",
    "            bachOccupation = col.get_text()      \n",
    "            bachOccupation = bachOccupation.encode('utf8')\n",
    "            \n",
    "        numCol += 1\n",
    "    \n",
    "    #hard-code data for Bachelors who don't have own wiki page\n",
    "    if (\"Grant\" in bachName):\n",
    "        bachHometown = \"London, UK\"\n",
    "        bachAge = \"27\"\n",
    "    if (\"Womack\" in bachName):\n",
    "        bachHometown = \"Austin, Texas\"\n",
    "        bachAge = \"37\" \n",
    "    if (\"Flajnik\" in bachName):\n",
    "        bachHometown = \"Sonoma, California\"\n",
    "        bachAge = \"28\" \n",
    "    if (\"Soules\" in bachName):\n",
    "        bachHometown = \"Arlington, Iowa\"\n",
    "        bachAge = \"33\"\n",
    "    if (\"Palmer\" in bachName):\n",
    "        bachHometown = \"Toronto, Ontario\"\n",
    "    \n",
    "    #add info to bachelor dictionary\n",
    "    bachelorDict['name'] = bachName  \n",
    "    bachelorDict['age'] = bachAge\n",
    "    bachelorDict['hometown'] = bachHometown\n",
    "    bachelorDict['occupation'] = bachOccupation\n",
    "    bachelorDict['elimination'] = \"bachelor\"\n",
    "    bachelorDict['season'] = int(seasonNum.encode('utf8'))\n",
    "    \n",
    "    #add bachelor dictionary to list of all dicts for all people in all seasons\n",
    "    listAllDicts.append(bachelorDict)         \n",
    "    \n",
    "    numRow +=1  #get next bachelor from wiki table\n",
    "    \n",
    "    \n",
    "#Non-wiki Data sources:\n",
    "#Grant: http://www.realitytvworld.com/news/matt-grant-dishes-on-upcoming-the-bachelor-london-calling-finale-7066.php\n",
    "#Womack: http://www.people.com/people/article/0,,20429663,00.html\n",
    "#Soules:http://www.people.com/article/chris-soules-new-bachelor-in-love"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For earlier seasons, Wikipedia does not have the data - so we look to other websites to add in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get data for Seasons 2, 4 and 6.\n",
    "#\n",
    "# Despite much effort, I could not get the text from the'realitytvword.com' sources below.\n",
    "# The Beautiful Soup elements did not match the \"inspect element\" html tags.  \n",
    "# (Oddly, I was able to scrape from 'realitytv.about.com' for season 8 - see below.)\n",
    "#\n",
    "# I tried the following suggestions, but they did not work:\n",
    "#    https://www.reddit.com/r/learnpython/comments/2nqhzw/how_come_a_websites_page_source_html_is_different/\n",
    "#    http://stackoverflow.com/questions/26913316/beautiful-soup-doesnt-get-full-webpage\n",
    "# To keep on schedule, I hand-entered the data. If time permits, I will come back to this.\n",
    "#\n",
    "# SEASON 2 SOURCES\n",
    "# 1) http://www.realitytvworld.com/#$$nxtmgs&&BYitVosLEeWjwgrBiYTF8Q$$\n",
    "# 2) winner: http://www.realitywanted.com/shows/the-bachelor/season-2\n",
    "#\n",
    "# SEASON 4 SOURCES\n",
    "# 1) http://www.realitytvworld.com/#$$nxtgih&&Dvr38or5EeW1VRL/9wgFGw$$\n",
    "# 2) http://draheid.com/archives/bachelor4/messages/1452262/1105037.html\n",
    "# 3) winner: http://www.realitywanted.com/shows/the-bachelor/season-4\n",
    "#\n",
    "# SEASON 6 SOURCES\n",
    "# 1) Source:(looks different in Safari versus Chrome) http://www.realitytvworld.com/#$$nxt6je&&9hhYJIraEeWixQqIPWP/qw$$\n",
    "# 2) Alt. Source: http://www.realitytvworld.com/news/abc-releases-identities-of-sixth-bachelor-edition-bachelorettes-2880.php\")\n",
    "# 3) FYI, source without occupations, but with pictures: \n",
    "# (http://community.realitytvworld.com/cgi-sys/cgiwrap/rtvw2/community/dcboard.cgi?az=printer_format&om=894&forum=DCForumID42)\n",
    "# 4) Source of winner name: http://www.realitywanted.com/shows/the-bachelor/season-6\n",
    "# 5) Mary Delgado source: http://www.sptimes.com/2003/09/26/Tampabay/No_wedding_bells__jus.shtml\n",
    "#\n",
    "\n",
    "#make array of data for contestants in Season 2\n",
    "season2 = [\"Heather, a 23-year-old sales coordinator who currently resides in Walnut Creek, CA\",  \n",
    "\"Lori, a 26-year-old public relations representative who currently resides in Dallas, TX\",  \n",
    "\"Heather, a 30-year-old flight attendant who currently resides in Watauga, TX\",  \n",
    "\"Amber, a 25-year-old therapist who currently resides in Chapel Hill, NC\",  \n",
    "\"Cari, a 28-year-old elementary school teacher who currently resides in Granite City, IL\",  \n",
    "\"Christy, a 24-year-old radiologic technologist who currently resides in Avondale, AZ\",  \n",
    "\"Hayley, a 28-year-old store manager who currently resides in Dana Point, CA\",  \n",
    "\"Camille, a 29-year-old actress/model who currently resides in Los Angeles, CA\",  \n",
    "\"Kyla Faye, a 22-year-old recording artist who currently resides in Midvale, UT\",  \n",
    "\"Erin, a 25-year-old national magazine who currently resides in Chester, PA\",  \n",
    "\"Frances, a 30-year-old strategic planning analyst who currently resides in San Francisco, CA\",  \n",
    "\"Dana, a 24-year-old radio sales who currently resides in Beverly Hills, CA\",  \n",
    "\"Merrilee, a 27-year-old teacher who currently resides in Forked River, NJ\",  \n",
    "\"Suzi, a 27-year-old communications specialist who currently resides in Richmond, VA\",  \n",
    "\"Anindita, a 27-year-old attorney who currently resides in New York, NY\",  \n",
    "\"Fatima, a 22-year-old student who currently resides in Long Beach, CA\",  \n",
    "\"Helene Eksterowicz, a 27-year-old school psychologist who currently resides in Glouchester, NJ\",  \n",
    "\"Brooke Nicole, a 22-year-old student who currently resides in Tuscaloosa, AL\",  \n",
    "\"Liangy, a 30-year-old paralegal who currently resides in Coral Gables, FL\",  \n",
    "\"Erin, a 23-year-old interior designer who currently resides in Houston, TX\",  \n",
    "\"Suzanne, a 32-year-old flight attendant who currently resides in Redondo Beach, CA\",  \n",
    "\"Angela, a 26-year-old registered nurse who currently resides in Kansas City, MO\",  \n",
    "\"Shannon, a 25-year-old graphic artist who currently resides in Hicksville, NY\",  \n",
    "\"Christi Diane, a 23-year-old financial advisors asst. who currently resides in Eagle, ID\",  \n",
    "\"Gwen, a 31-year-old executive recruiter who currently resides in Chester Springs, PA\"] \n",
    "\n",
    "\n",
    "#make array of data for contestants in Season 4\n",
    "season4= [\"Brooke, a 24-year-old Teacher who currently resides in Bartlett, TN\", \n",
    "\"Lee-Ann, a 24-year-old Second Grade Teacher who currently resides in  Athens, GA\", \n",
    "\"Shea, a 25-year-old Firefighter who currently resides in Shreveport, LA\", \n",
    "\"Mary, a 35-year-old Sales Manager who currently resides in Tampa, FL\", \n",
    "\"Lindsay, a 23-year-old Professional Dancer who currently resides in Los Angeles, CA\", \n",
    "\"Estella Gardinier, a 27-year-old Mortgage Broker who currently resides in Beverly Hills, CA\", \n",
    "\"Lanah, a 27-year-old Event Coordinator who currently resides in Poolesville, MD\", \n",
    "\"Jenny, a 30-year-old Marketing Director who currently resides in Austin, TX\", \n",
    "\"Kristi, a 24-year-old Loan Processor who currently resides in Chicago, IL\", \n",
    "\"Lindsay, a 25-year-old Pharmaceutical Sales who currently resides in Mauldin, SC\", \n",
    "\"Shelly, a 26-year-old Pharmaceutical Sales who currently resides in Wanwatosa, WI\", \n",
    "\"Kelly Jo, a 23-year-old Director of Community Relations who currently resides in  Kalamazoo, MI\", \n",
    "\"Antoinette, a 30-year-old Senior Account Manager who currently resides in Philadelphia, PA\", \n",
    "\"Stacey, 26-year-old a Hair Stylist who currently resides in  Massillon, OH\", \n",
    "\"Heather, a 24-year-old Recent College Graduate who currently resides in Chicago, IL\",\n",
    "\"Meredith, a 29-year-old Model/ Makeup Artist who currently resides in West Hollywood, CA\", \n",
    "\"Misty, a 23-year-old Radio Promotions Assistant who currently resides in Dallas, TX\", \n",
    "\"Christine, a 24-year-old Administrative Assistant who currently resides in Corona, CA\", \n",
    "\"Jenn, a 26-year-old Elementary School Teacher who currently resides in La Jolla, CA\", \n",
    "\"Leona, a 25-year-old Realtor's Assistant who currently resides in Chicago, IL\", \n",
    "\"Samantha, a 25-year-old Kitchen Designer who currently resides in Chicago, IL\", \n",
    "\"Julie, a 29-year-old Sales/ Modeling who currently resides in Louisville, KY\", \n",
    "\"Karin, a 32-year-old Mortgage Consultant who currently resides in Brooklyn Park, MN\", \n",
    "\"Lauren, a 24-year-old Retail Buyer who currently resides in Redondo Beach, CA\", \n",
    "\"Darla, a 26-year-old Attorney who currently resides in Gainesville, FL\"] \n",
    "\n",
    "#make array of data for contestants in Season 6\n",
    "season6 = [\"Abby, a 29-year-old acrobat who currently resides in Henderson, NV\", \n",
    "\"Alma Rubenstein, a 35-year-old cafe owner who currently resides in Astoria, OR\",\n",
    "\"Amanda, a 27-year-old cosmetics buyer who currently resides in New York, NY\", \n",
    "\"Amy, a 27-year-old marketing consultant who currently resides in San Diego, CA\", \n",
    "\"Andrea, a 33-year-old dental hygienist who currently resides in Denver, CO\", \n",
    "\"Ashley, a 31-year-old teacher who currently resides in Santa Barbara, CA\", \n",
    "\"Carolyn, a 36-year-old financial advisor who currently resides in Tulsa, OK\", \n",
    "\"Cheresse, a 31-year-old advertising director who currently resides in St. Louis, MO\", \n",
    "\"Cynthia, a 37-year-old charity foundations director who currently resides in Hermosa Beach, CA\", \n",
    "\"Elizabeth, a 28-year-old in pharmaceutical sales who currently resides in Chicago, IL\", \n",
    "\"Jayne, a 37-year-old dog groomer, who currently resides in Key Largo, FL\",\n",
    "\"Jennifer, a 31-year-old account executive who currently resides in Seattle, WA\", \n",
    "\"Kelly, a 34-year-old actress who currently resides in Beverly Hills, CA\", \n",
    "\"Kerry, a 31-year-old nurse who currently resides in San Francisco, CA\", \n",
    "\"Kristie, a 32-year-old bar owner who currently resides in Windsor, Canada\", \n",
    "\"Kristin, a 27-year-old office manager who currently resides in Pensacola, FL\", \n",
    "\"Krysta, a 28-year-old financial analyst who currently resides in Oklahoma City, OK\", \n",
    "\"Leina, a 28-year-old advertising associate who currently resides in Chula Vista, CA\", \n",
    "\"Lisa, a 33-year-old teacher who currently resides in West Palm Beach, FL\", \n",
    "\"Melinda, a 39-year-old photographer who currently resides in Nashville, TN\", \n",
    "\"Natalie, a 34-year-old in retail sales who currently resides in Santa Monica, CA\", \n",
    "\"Nicole, a 28-year-old executive recruiter who currently resides in Libertyville, IL\", \n",
    "\"Susie, a 32-year-old insurance broker who currently resides in Hollywood, CA\", \n",
    "\"Tanya, a 31-year-old teacher who currently resides in Plano, Texas\", \n",
    "\"Wende, a 28-year-old model who currently resides in Austin, Texas\",\n",
    "\"Mary Delgado, a 35-year-old real estate agent who currently resides in Tampa Bay, FL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add contestant data for seasons 2, 4 and 6 to dictionary 'contestantDict'.\n",
    "#\n",
    "# param : array of strings with contestant data\n",
    "# param : season number\n",
    "# param : winner name\n",
    "def addNonWikiData(contestantArray, seasonNum, winnerName):\n",
    "    for line in contestantArray:\n",
    "        firstComma = line.index(',')                    #parse string\n",
    "        startAge = line.index(\" a \")\n",
    "        jobTag = \"year-old \"     \n",
    "        startJob = line.index(jobTag)\n",
    "        homeTag = \"currently resides in \"\n",
    "        startHome = line.index(homeTag)\n",
    "        contestantDict = dict()                        #init new dict for contestant   \n",
    "        contestantDict['name'] = line[:firstComma]     #put field data into dictionary\n",
    "        contestantDict['age'] = line[startAge+3:startAge+5]\n",
    "        contestantDict['hometown'] = line[startHome + len(homeTag):]\n",
    "        contestantDict['occupation'] = line[startJob + len(jobTag):line.index(\"who\")-1]\n",
    "        contestantDict['season'] = seasonNum\n",
    "    \n",
    "        if (winnerName in line):                       #if this is the Winner,\n",
    "            contestantDict['elimination'] = \"Winner\"   #add 'winner' to 'elimination' field\n",
    "        else:\n",
    "            contestantDict['elimination'] = \"unknown\"\n",
    "        listOfContestantDicts.append(contestantDict)   #add dict to list of dicts for this season\n",
    "        listAllDicts.append(contestantDict)           #add dict to list of all dicts in all seasons\n",
    "           \n",
    "    seasonsDict[seasonNum] = listOfContestantDicts     #key = season, val=list of contestant dicts\n",
    "    \n",
    "    \n",
    "#add contestant data for seasons 4 and 6 to the contestant dictionary\n",
    "addNonWikiData(season2, 2, \"Eksterowicz\")\n",
    "addNonWikiData(season4, 4, \"Gardinier\")\n",
    "addNonWikiData(season6, 6, \"Delgado\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get data for Season 8, add to dictionary\n",
    "\n",
    "#get site with season 8 contestants, make soup element\n",
    "seasonEight = requests.get(\"http://realitytv.about.com/od/thebachelor8/ig/Ladies-of-The-Bachelor--Paris/\")                #get site\n",
    "season8= BeautifulSoup(seasonEight.text, \"html.parser\")\n",
    "\n",
    "#get the table cell that has links to each episode\n",
    "eight = season8.find(\"body\", attrs={\"id\":\"imagegalleryIndexPage\"})\n",
    "eight = eight.find(\"main\", attrs={\"id\":\"main\"})\n",
    "eight = eight.find(\"div\", attrs={\"class\":\"container\"})\n",
    "eight = eight.find_all(\"div\", attrs={\"class\":\"row\"})[1]\n",
    "eight = eight.find(\"div\", attrs={\"class\":\"col col-11\"}).find(\"div\", attrs={\"class\":\"row\"})\n",
    "eight = eight.find(\"div\", attrs={\"class\":\"col col-8\"})\n",
    "eight = eight.find(\"div\", attrs={\"class\":\"content widget gallery-index-content\"})\n",
    "eight = eight.find(\"ul\")\n",
    "\n",
    "urls8 = []                       #list of urls for season 8 contestant pages\n",
    "for item in eight.find_all(\"li\", attrs={\"itemtype\":\"http://schema.org/ImageObject\"}):#for each contestant in list of season 8 contestants\n",
    "    url8 = item.find(\"a\")        #get url tag\n",
    "    if url8 is not None:         #if has url link, get url \n",
    "        urls8.append(\"\\\"http://realitytv.about.com\" + url8.get(\"href\") + \"\\\"\")\n",
    "\n",
    "#add contestant site leftover from next page\n",
    "urls8.append(\"\\\"http://realitytv.about.com/od/thebachelor8/ig/Ladies-of-The-Bachelor--Paris/Shiloh-of-The-Bachelor--Paris.htm\\\"\")  \n",
    "\n",
    "\n",
    "cont8Sites = []                  #list of soup objects for season 8 contestant sites\n",
    "for link in urls8:\n",
    "    site8 = requests.get(link[1:-1]) \n",
    "    soup8 = BeautifulSoup(site8.text, \"html.parser\") #get soup element\n",
    "    cont8Sites.append(soup8)     #add soup element to list     \n",
    "\n",
    "for cont8 in cont8Sites:         #for each soup element (one per contestant site),\n",
    "    c8 = cont8.find(\"body\", attrs={\"id\":\"imagegalleryPage\"}) #find data\n",
    "    c8 = c8.find(\"main\", attrs={\"class\":\"slab\"})\n",
    "    c8 = c8.find(\"div\", attrs={\"class\":\"container\"})\n",
    "    c8 = c8.find_all(\"div\", attrs={\"class\":\"row\"})[1]\n",
    "    c8 = c8.find(\"div\", attrs={\"class\":\"col col-11\"})\n",
    "    c8 = c8.find(\"div\", attrs={\"id\":\"contentIntro\"})\n",
    "    c8 = c8.find(\"div\", attrs={\"class\":\"row\"})\n",
    "    c8 = c8.find(\"div\", attrs={\"class\":\"col col-6\"})\n",
    "    c8 = c8.find(\"div\", attrs={\"class\":\"muted subheading\"}).getText()\n",
    "    \n",
    "    contestantDict = dict()     #init new dict for contestant\n",
    "\n",
    "    #get name\n",
    "    firstComma = c8.index(',')\n",
    "    contestantDict['name'] = c8[:firstComma]\n",
    "        \n",
    "    #get age\n",
    "    substrC8 = c8[firstComma+2:]\n",
    "    secondComma = substrC8.index(',')\n",
    "    contestantDict['age'] = substrC8[:secondComma]\n",
    "        \n",
    "    #get hometown\n",
    "    hometag = \"resides in \"\n",
    "    if (hometag not in c8):\n",
    "        hometag = \"living in \"\n",
    "    homeIndex = c8.index(hometag)\n",
    "    contestantDict['hometown'] = c8[(homeIndex+len(hometag)):-1]\n",
    "        \n",
    "    #get job\n",
    "    jobtag = \"is a \"\n",
    "    endjobtag = \" who\"\n",
    "    if (\"is an\" in c8):\n",
    "        jobtag = \"is an \"\n",
    "    if(\"works in\" in c8):   #has format \"Tara, 23, works in X and currently resides in Y\"\n",
    "        jobtag = \"works in \"\n",
    "        endjobtag = \" and currently resides\"\n",
    "    if(\"is the\" in c8):\n",
    "        jobtag = \"is the \"\n",
    "        endjobtag = \" and currently resides\"\n",
    "    if (endjobtag not in c8):\n",
    "        endjobtag = \" living in\"\n",
    "    contestantDict['occupation'] = c8[(c8.index(jobtag)+len(jobtag)):(c8.index(endjobtag))]   #add name to dict\n",
    "\n",
    "    #get elimination week\n",
    "    if (\"Sarah Stone\" in name):         #hard-code season 8 winner\n",
    "        contestantDict['elimination'] = \"Winner\"\n",
    "    else:\n",
    "        contestantDict['elimination'] = \"unknown\"\n",
    "    \n",
    "    #add season\n",
    "    contestantDict['season'] = 8\n",
    "        \n",
    "    #add dict to list of dicts\n",
    "    #if (contestantDict not in newList):\n",
    "    listOfContestantDicts.append(contestantDict) #add dict to list of dicts in this season\n",
    "    listAllDicts.append(contestantDict)  #add dict to list of all dicts in ALL seasons\n",
    "\n",
    "seasonsDict[8] = listOfContestantDicts  #key = season num, val=list of contestant dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Save to Disk\n",
    "\n",
    "#import json\n",
    "#fd = open(\"tempdata/seasonsDict.json\", \"w\")   #save dictionary to disk\n",
    "#json.dump(seasonsDict, fd)\n",
    "#fd.close()\n",
    "\n",
    "#del seasonsDict\n",
    "#with open(\"tempdata/seasonsDict.json\", \"r\") as fd: \n",
    "#    seasonsDict = json.load(fd)               #reload "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>elimination week</th>\n",
       "      <th>group_dates</th>\n",
       "      <th>hometown</th>\n",
       "      <th>individual_dates</th>\n",
       "      <th>name</th>\n",
       "      <th>occupation</th>\n",
       "      <th>roses_from_group_dates</th>\n",
       "      <th>roses_from_individual_dates</th>\n",
       "      <th>season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Winner</td>\n",
       "      <td>2</td>\n",
       "      <td>Dallas, Texas</td>\n",
       "      <td>1</td>\n",
       "      <td>Melissa Rycroft</td>\n",
       "      <td>Sales Representative</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>Runner-up</td>\n",
       "      <td>1</td>\n",
       "      <td>Grand Rapids, Michigan</td>\n",
       "      <td>1</td>\n",
       "      <td>Molly Malaney</td>\n",
       "      <td>Department Store Buyer</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>Peace River, Alberta</td>\n",
       "      <td>1</td>\n",
       "      <td>Jillian Harris</td>\n",
       "      <td>Interior Designer</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>Carlsbad, California</td>\n",
       "      <td>0</td>\n",
       "      <td>Naomi Rose Crespo</td>\n",
       "      <td>Flight Attendant</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Huntsville, Alabama</td>\n",
       "      <td>2</td>\n",
       "      <td>Stephanie Hogan</td>\n",
       "      <td>Single Mother &amp; Medical Marketing Rep.</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  age elimination week  group_dates                hometown  individual_dates               name                              occupation  roses_from_group_dates  roses_from_individual_dates  season\n",
       "0  25           Winner            2           Dallas, Texas                 1    Melissa Rycroft                    Sales Representative                       0                            1      13\n",
       "1  24        Runner-up            1  Grand Rapids, Michigan                 1      Molly Malaney                  Department Store Buyer                       1                            1      13\n",
       "2  29                7            2    Peace River, Alberta                 1     Jillian Harris                       Interior Designer                       0                            0      13\n",
       "3  24                6            3    Carlsbad, California                 0  Naomi Rose Crespo                        Flight Attendant                       1                            0      13\n",
       "4  34                5            0     Huntsville, Alabama                 2    Stephanie Hogan  Single Mother & Medical Marketing Rep.                       0                            2      13"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert list of contestant dictionaries to a pandas dataframe.\n",
    "#\n",
    "# Note: 'listAllDicts' has seasons = [2,4,5,6,8,9,10,11,12,13,14,15,16,17,18,19] \n",
    "# Here, we make a dataframe of Seasons 13 forward.\n",
    "#\n",
    "\n",
    "cDicts = []                  \n",
    "for l in listAllDicts:\n",
    "    if (l['season'] > 12):       #for seasons 13 forward,\n",
    "        d={}\n",
    "        d['name']=l['name']\n",
    "        d['age']=l['age']\n",
    "        d['hometown']=l['hometown']\n",
    "        d['occupation']=l['occupation']\n",
    "        d['elimination week']=l['elimination']  #for bachelors, value will be 'bachelor'\n",
    "        if 'group_dates' in l and 'individual_dates' in l:\n",
    "            d['group_dates'] = l['group_dates']\n",
    "            d['individual_dates'] = l['individual_dates']\n",
    "            d['roses_from_group_dates'] = l['roses_from_group_dates']\n",
    "            d['roses_from_individual_dates'] = l['roses_from_individual_dates']\n",
    "        else:\n",
    "            d['group_dates'] = 0\n",
    "            d['individual_dates'] = 0\n",
    "            d['roses_from_group_dates'] = 0\n",
    "            d['roses_from_individual_dates'] = 0\n",
    "        d['season']=l['season']\n",
    "        cDicts.append(d)\n",
    "        \n",
    "contestantDF = pd.DataFrame(cDicts)\n",
    "contestantDF.drop_duplicates()  #drop duplicates, just in case\n",
    "contestantDF.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('competition_data.json', 'w') as fp:\n",
    "    json.dump(weeklyCompData, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
